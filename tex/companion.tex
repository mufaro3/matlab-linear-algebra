\documentclass[12pt]{article}
\usepackage{parskip, multicol}
\usepackage{amssymb, amsmath, amsthm, physics}
\usepackage{hyperref}

\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\exercise}[1]{\textbf{EXERCISE #1}\label{#1}}
\newcommand{\theorem}[2]{\textbf{THEOREM #1.} #2}
\newcommand{\conjecture}[1]{\textbf{CONJECTURE:} #1}
\newcommand{\notes}{\textbf{NOTES}}
\newcommand{\rref}{\text{RREF}}
\newcommand{\eig}{\text{eig }}
\newcommand{\ct}[1]{\mat{\overline{#1}}^{T}}
\newcommand{\nullsp}{\text{null }}
\newcommand{\nullility}{\text{nulllility }}
\newcommand{\rowsp}{\text{rowsp }}
\newcommand{\colsp}{\text{colsp }}
\newcommand{\eigsp}{\text{eigsp }}
\newcommand{\subtheorem}[1]{\textbf{SUBTHEOREM:} #1}
\newcommand{\basis}{\text{basis }}
\newcommand{\set}[2]{\left\{\left.#1\right|#2\right\}}
\newcommand{\vspan}{\text{span }}

\newcommand{\rot}{\text{rot }}
\newcommand{\dom}{\text{dom }}
\newcommand{\range}{\text{range }}

\begin{document}

\textbf{\Large COMPANION NOTES AND SOLUTIONS}

by Mufaro Machaya

\tableofcontents
\newpage

\section{Chapter 1: Matrices}

\subsection{Labs 0 and 1: Introduction and Matrix Basics}

\notes

\begin{enumerate}
\item Matrix addition is commutative, so addition in any order produces the same result.
\begin{equation}
  \mat{A} + \mat{B} = \mat{B} + \mat{A}
\end{equation}
\item Scalar multiplication is applied on all elements of the given matrix as a regular multiplication. For example, for $3 \times 3$ matrix $\mat{A}$ and scalar $s$:
\begin{equation}
    s\mat{A} =
\begin{pmatrix} 
  sA_{11} & sA_{12} & sA_{13} \\
  sA_{21} & sA_{22} & sA_{23} \\
  sA_{31} & sA_{32} & sA_{33}
\end{pmatrix}.
\end{equation}
\item A transposition is essentially swapping your rows and columns (do not, however, mistake it for a rotation), so $2 \times 3$ matrix $\mat{A}$ would produce $3 \times 2$ matrix $\mat{A}^{-1}$:
\begin{equation}
  \begin{pmatrix}
    A_{11} & A_{12} & A_{13} \\
    A_{21} & A_{22} & A_{23}
  \end{pmatrix}^{T} =
  \begin{pmatrix}
    A_{11} & A_{21} \\
    A_{12} & A_{22} \\
    A_{13} & A_{23}
  \end{pmatrix}
\end{equation}
\item Matrix multiplication essentially computes the dot products of the rows of the first matrix with the columns of the second matrix, thus why the number of rows of the first matrix must equal the number of columns of the second. For example, for $2 \times 3$ matrix $\mat{A}$ and $3 \times 2$ matrix $\mat{B}$, the resulting matrix would be
\begin{equation*}
  \mat{A} \mat{B} =
\begin{pmatrix}
  \mat{A}_{r=1} \cdot \mat{B}_{c=1} & \mat{A}_{r=1} \cdot \mat{B}_{c=2} \\
  \mat{A}_{r=2} \cdot \mat{B}_{c=1} & \mat{A}_{r=2} \cdot \mat{B}_{c=2}
\end{pmatrix}
  \footnote{$M_{r=m}$ denotes ``the vector comprised of the elements of row $m$ in matrix $\mat{M}$'' and $M_{c=n}$ denotes ``the vector comprised of the elements of column $n$ in matrix $\mat{M}$.''}
\end{equation*}
\begin{equation}
    =
\begin{pmatrix}
  A_{11} B_{11} + A_{12} B_{21} + A_{13} B_{31} &
  A_{11} B_{12} + A_{12} B_{22} + A_{13} B_{32} \\
  A_{21} B_{11} + A_{22} B_{21} + A_{23} B_{31} &
  A_{21} B_{12} + A_{22} B_{22} + A_{23} B_{32}
\end{pmatrix}
\end{equation}
\item Matrix multiplication is not commutative, so multiplying matrices in different orders will produce different results.
\begin{equation}
  \mat{A} \mat{B} \neq \mat{B} \mat{A}
\end{equation}
\item Matrix addition and scalar multiplication are commutative with transposition, but multiplication is not.
\begin{align}
  (\mat{A} + \mat{B})^T &= \mat{A}^T + \mat{B}^T \\
  (s\mat{A})^T &= s\mat{A}^T \\
  (\mat{A}\mat{B})^T &\neq \mat{A}^T \mat{B}^T
\end{align}
\item The trace of a matrix is the sum of the elements on its main diagonal.
\item The trace is commutative with matrix addition (i.e., $\tr(\mat{A} + \mat{B}) = \tr(\mat{A}) + \tr(\mat{B})$), but non-commutative with matrix multiplication. 
\item Matrix multiplication within trace in either order will produce the same result,
\begin{equation}
\tr(\mat{AB}) = \tr(\mat{BA})
\end{equation}
\end{enumerate}

\subsection{Lab 2: A Matrix Representation of Linear Systems}

\exercise{2(g)}

Given
\begin{equation*}
  \mat{M} = \begin{pmatrix}
    1 & 2 & 0 \\
    0 & 0 & 3 \\
    0 & 1 & 0
  \end{pmatrix},
\end{equation*}
we can produce $\mat{M} \to \mat{I}_3$ via:
\begin{enumerate}
\item Divide row $3$ by 3.
\item Add multiples of -2 from row 2 to row 1.
\item Swap rows 2 and 3\footnote{This was originally wrong and placed first. You need to swap last because matrix multiplication is right-associative, so the rightmost operation occurs first, not last. In other words, a ``correct'' way to approach this would be to do the entire process in reverse when multiplying.}.
\end{enumerate}
This effect can be produced by the following elementary matrices. 

The swap matrix can be created from emulation with $I_3$:
\begin{equation*}
\mat{E}_{swap} = 
\begin{pmatrix}
 1 & 0 & 0 \\
 0 & 0 & 1 \\
 0 & 1 & 0
\end{pmatrix},
\end{equation*}
similarly, the division matrix would be
\begin{equation*}
\mat{E}_{div} =
\begin{pmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & \frac{1}{3}
\end{pmatrix},
\end{equation*}
and lastly, the additive matrix would be
\begin{equation*}
\mat{E}_{add} =
\begin{pmatrix}
1 & -2 & 0 \\
0 & 1  & 0 \\
0 & 0  & 1
\end{pmatrix}.
\end{equation*}
Altogether, the following multiplication should hold true:
\begin{equation*}
\mat{E}_{div} \mat{E}_{add} \mat{E}_{swap} \mat{M} = \mat{I}_{3}
\end{equation*}
\newpage

\exercise{2(h)}

Given
\begin{align*}
x + 2y &= 4 \\
3z &= 6 \\
y &= 8,
\end{align*}
we can produce the following coefficient matrix $\mat{A}$ and constant vector $\mat{B}$:
\begin{equation*}
\mat{A} = 
\begin{pmatrix} 
1 & 2 & 0 \\ 
0 & 0 & 3 \\ 
0 & 1 & 0 
\end{pmatrix} 
\hspace{0.10\linewidth} 
\mat{B} = 
\begin{pmatrix} 
4 \\ 6 \\ 8
\end{pmatrix},
\end{equation*}
and given that $\mat{A} = \mat{M},$ from exercise 2(g) the solutions can be calculated by performing the RREF calculations on $\mat{B}$ to produce solution vector $\mat{S}$:
\begin{equation*}
\mat{E}_{div} \mat{E}_{add} \mat{E}_{swap} \mat{B} = \mat{S},
\end{equation*}
yielding
\begin{equation*}
\mat{S} = \begin{pmatrix} -12 \\ 8 \\ 2 \end{pmatrix},
\end{equation*}
showing that $x=-12, y = 8, z=2$.

\exercise{2(i)}

This isn't solvable because the best RREF form is
\begin{equation*}
\mat{[M | B]} = 
\begin{pmatrix}
1 & 0 & -6 & 4 \\
0 & 1 &  3 & 6 \\
0 & 0 &  0 & 6 
\end{pmatrix}
\end{equation*}
or, in other words, equations (2) and (3) contradict each other.

\exercise{2(j)}

The initial state of the system is
\begin{equation*}
\begin{pmatrix}
1 & 2 & 0 & 4 \\
0 & 1 & 3 & 6 \\
0 & 2 & 6 & 12
\end{pmatrix},
\end{equation*}
and the RREF of this system becomes
\begin{equation*}
\begin{pmatrix}
1 & 0 & -6 & -8 \\
0 & 1 & 3 & 6 \\
0 & 0 & 0 & 0
\end{pmatrix}.
\end{equation*}
Again, this system is not solvable as equations 2 and 3 are equivalent, resulting in this system not containing enough information to be adequately solved, but the reduced echelon form of the coefficient matrix is the same as in exercise 2(i).

\subsection{Lab 3: Powers, Inverses, and Special Matrices}

\notes

\begin{enumerate}
\item A matrix multiplied by its transposition will produce a gram matrix which is symmetric (does not change on transposition).
\begin{equation}
  \mat{M}^T \mat{M} = (\mat{M}^T \mat{M})^T \neq \mat{M} \mat{M}^T = (\mat{M} \mat{M}^T)^T
\end{equation}
\item A matrix multiplied by its inverse, in any order, will produce an identity matrix.
\begin{equation}
  \mat{M} \mat{M}^{-1} = \mat{M}^{-1} \mat{M} = \mat{I}_{n}
\end{equation}
\item A matrix inverted twice produces itself.
\begin{equation}
(\mat{M}^{-1})^{-1} = \mat{M}
\end{equation}
\item Matrix multiplication is not commutative with inversion, however, they are commutative in alternating order.
\begin{equation}
(\mat{AB})^{-1} = \mat{B}^{-1} \mat{A}^{-1} \neq (\mat{BA})^{-1} = \mat{A}^{-1} \mat{B}^{-1}
\end{equation}
\item Matrix inversion is commutative with transposition.
\begin{equation}
(\mat{M}^T)^{-1} = (\mat{M}^{-1})^{T}
\end{equation}
\item A square matrix $\mat{M}$ is
\begin{enumerate}
\item Symmetric when $\mat{M} = \mat{M}^T$
\item Diagonal when $\mat{M}_{ij} = 0$ if $i \neq j$
\item Upper triangular when $\mat{M}_{ij} = 0$ if $i > j$ or lower triangular when $i < j$.
\end{enumerate}
\item Assuming a matrix to be square, a matrix plus its transposition is symmetric.
\begin{equation}
\mat{M} + \mat{M}^T = (\mat{M} + \mat{M}^T)^T
\end{equation}
\end{enumerate}

\theorem{1}{The inverse of an elementary matrix is an elementary matrix.}

\begin{proof}
There are three elementary operations to be considered: row swaps, multiplication/division by a scalar, and addition/subtraction of a row (by a scalar). 

Let $\mat{E}_{swap}$ be the elementary matrix produced by a swap. By gaussian elimination, $\mat{E}_{swap}$ is its own inverse,
$$\mat{E}_{swap}^{-1} = \mat{E}_{swap},$$
as it can be undone (returned to the identity matrix $\mat{I}$) by repeating the swap that produced it. 

Let $\mat{E}_{scale}$ be the elementary matrix produced by the multiplication of any scalar $s$. This multiplication by $s$ can be undone by division by $s$, so $\mat{E}^{-1}_{scale}$ would be the elementary matrix produced by the divison of $s$. 

Let $\mat{E}_{add}$ be the elementary matrix produced by the addition by any scalar multiple $s$ from one row to another. As addition can be undone by subtraction, the addition by a scalar multiple $s$ can be undone by the subtraction of the same values.
\end{proof}

\theorem{2}{If $\mat{A}$ is invertible then the reduced row echelon form of $\mat{A}$ is $\mat{I}$.}

\begin{proof}
If $\mat{A} \in \mathbb{R}^{n \times n}$ is invertible, then there exists some composite matrix $\mat{E}$ as the product of elementary operations that satisfies the following two equations:
\begin{equation*}
\mat{EA} = \mat{I}, \hspace{0.25\textwidth} \mat{EI} = \mat{A}^{-1}.
\end{equation*}
These two equations correspond to the Gauss-Jordan elimination process, showing the transformation results on the left and right sides, and the left equation will reduce $\mat{A}$ into its reduced row echelon form. Therefore, the RREF form of $\mat{A}$ must be $\mat{I}$.
\end{proof}

\theorem{3}{If the reduced row echelon form of $\mat{A}$ is $\mat{I}$, then $\mat{A}$ is invertible.}

\begin{proof}
As shown in theorem 2, for $\mat{A} \in \mathbb{R}^{n \times n}$ to satisfy $\rref(\mat{A}) = \mat{I}$, there must be a composite matrix $\mat{E} \in \mathbb{R}^{n \times n}$ such that $\mat{EA} = \mat{I}$, and therefore, this same matrix will simultaneously produce $\mat{EI} = \mat{A}^{-1}$ by Gauss-Jordan elimination, so $\mat{A}$ must be invertible.
\end{proof}

\theorem{4}{$\mat{A}$ is a square invertible matrix if and only if $\mat{A}$ can be written as the product of elementary matrices.}

\begin{proof}
Again, for $\mat{A} \in \mathbb{R}^{n \times n}$ to be invertible, it must satisfy $\mat{EA} = \mat{I}$ and $\mat{EI} = \mat{A}^{-1}$ for some product matrix $\mat{E}$ of elementary matrices in form
$$\mat{E} = \mat{E}_{k} \mat{E}_{k-1} \dots \mat{E}_{2} \mat{E}_{1} = \left( \prod_{n=1}^{k} \mat{E}_{n} \right)^{\sim}.\footnote{The tilde denotes that this product is in reverse order.}\footnote{From here on out, further matricies will be described as "composite."}$$
Therefore, $\mat{A} = \mat{E}^{-1}$, and as the inverse of elementary matrices is elementary, $A$ inherently must be the product of elementary matrices if it is invertible. Furthermore, all composite matrices are inherently invertible because all elementary matrices are invertible, so altogehter, $\mat{A}$ is square and invertible if and only if its a composite product of elementary matrices. 
\end{proof}

\theorem{5}{If $\mat{A}$ is invertible then $\mat{A}^{k}$ is invertible for any natural number $k$.}

\begin{proof}
If $\mat{A} \in \mathbb{R}^{n \times n}$ is invertible, then $\mat{A} \mat{A}^{-1} = \mat{A}^{-1} \mat{A} = I$, so the multiplication between them is commutative, meaning that 
$$\mat{A}^{k} (\mat{A}^{-1})^{k} = (\mat{A} \mat{A}^{-1})^{k} = \mat{I}^{k} = \mat{I}.$$
\end{proof}

\theorem{6}{If $\mat{A}$ is symmetric so is $\mat{A}^{T}$.}

\begin{proof}
By the definition of symmetry, $\mat{A} = \mat{A}^{T}$, so $\mat{A}^{T}$ is symmetric.
\end{proof}

\theorem{7}{If $\mat{A}$ is a symmetric invertible matrix then $\mat{A}^{-1}$ is symmetric.}

\begin{proof}
Transposition is commutative with inversion, so $(\mat{A}^{T})^{-1} = (\mat{A}^{-1})^{T}.$ Therefore, if $\mat{A} = \mat{A}^{T}$ then $\mat{A}^{-1} = (\mat{A}^{T})^{-1} = (\mat{A}^{-1})^{T}$.
\end{proof}

\theorem{8}{If $\mat{A}$ and $\mat{B}$ are symmetric matrices of the same size then $\mat{A} + \mat{B}$ is symmetric.}

\begin{proof}
$\mat{A} = \mat{A}^{T}$ and $\mat{B} = \mat{B}^{T}$, so 
$$\mat{A} + \mat{B} = \mat{A}^{T} + \mat{B}^{T} = \mat{A}^{T} + \mat{B} = \mat{A} + \mat{B}^{T}.$$
Matrix multiplication is inherently commutative with transposition, so $(\mat{A} + \mat{B})^{T} = \mat{A}^{T} + \mat{B}^{T} = \mat{A} + \mat{B}$.
\end{proof}

\theorem{9}{If $\mat{A}$ and $\mat{B}$ are symmetric matrices of the same size, then $\mat{AB}$ is symmetric.}

\begin{proof}
As $\mat{A} = \mat{A}^{T}$ and $\mat{B} = \mat{B}^{T}$, then $\mat{AB} = \mat{A}^{T} \mat{B}^{T} = \mat{A}^{T} \mat{B} = \mat{A} \mat{B}^{T}$, and as matrix multiplication is commutative with transposition in a reverse order (i.e., $(\mat{AB})^{T} = \mat{B}^{T} \mat{A}^{T}$ and vice versa), because both $\mat{A}$ and $\mat{B}$ are symmetric, the matrix multiplication and transposition in this case are fully commutative, so $\mat{AB} = \mat{BA} = \mat{A}^{T} \mat{B}^{T} = (\mat{AB})^{T} = \mat{B}^{T} \mat{A}^{T} = (\mat{BA})^{T}$.
\end{proof}

\theorem{10}{If $\mat{A}$ is a square matrix then $\mat{A} + \mat{A}^{T}$ is symmetric.}

\begin{proof}
Transposition is commutative with addition, thus making it distributive: $(\mat{A} + \mat{A}^{T})^{T} = \mat{A}^{T} + (\mat{A}^{T})^{T} = \mat{A}^{T} + \mat{A}.$
\end{proof}

\theorem{11}{The sum of upper triangular matrices is upper triangular.}

\begin{proof}
This is intuitively true based on how matrix addition works. Let $\mat{A} \in \mathbb{R}^{n \times n}$ and $\mat{B} \in \mathbb{R}^{n \times n}$. All $A_{ij} + B_{ij} = 0$ where $i > j$ as $A_{ij} = 0$ and $B_{ij} = 0$. 
\end{proof}

\subsection{Lab 4: Graph Theory and Adjacency Matrices}

\notes

\begin{enumerate}
\item Graphs are comprised of nodes (points/places/etc) and edges (the relationships between the nodes, like paths/connections).
\item An edge and the node connected to it are called an indicent. Two nodes connected by an edge are considered adjacent.
\item Graphs can be represented as matrices in the form of an adjacency matrix, which are symmetric, square matrices such that indexing the matrix for the points in either order will produce the number of connections between the nodes at that depth, such as:
\begin{equation*}
\mat{M} = \begin{pmatrix}
0 & 1 & 1 & 0 \\
1 & 0 & 0 & 1 \\
1 & 0 & 0 & 1 \\
0 & 1 & 1 & 0
\end{pmatrix}.
\end{equation*}
For example, let $\mat{M}$ be a the adjacency matrix storing two points, $a$ and $b$ (where $a$ and $b$ each correspond to indices of the matrix respectively). The number of connections $n$ long (passing through $n$ edges) will be found at
\begin{equation}
\mat{M}^{n}(a,b) \hspace{0.125\textwidth} \text{and} \hspace{0.125\textwidth} \mat{M}^{n}(b,a).
\end{equation}  
\end{enumerate}

\subsection{Lab 5: Permutations and Determinants}

\notes

\begin{enumerate}
\item Given the number of options/variables, $n$, the number of permutations given $n$ will be $n!$.
\item The determinant of a $2 \times 2$ matrix is
\begin{equation}
\det \begin{pmatrix}
  a_{11} & a_{12} \\
  a_{21} & a_{22}
\end{pmatrix} = a_{11} a_{22} - a_{12} a_{21}.
\end{equation}
\item If $\det \mat{M} = 0$ then $\mat{M}$ is not invertible.
\item For scalar $k$ and $n \times n$ matrix $\mat{M}$, $\det ( k \mat{M} ) = k^{n} \det \mat{M}$
\item The determinant of the identity matrix, of any size, is always $1$.
\begin{equation}
\det \mat{I}_{n} = 1 \text{ for } n \in \{ 1, 2, \dots  \}
\end{equation}
\item The determinant of a matrix is equal to the determinant of its transposition. 
\begin{equation}
\det \mat{M} = \det \mat{M}^{T}
\end{equation}
\item Matrix multiplication is fully commutative with determinants, so
\begin{equation}
\det ( \mat{AB} ) = \det ( \mat{BA} ) = \det \mat{A} \det \mat {B}
\end{equation}
\item Again, matrix addition is commutative, so 
\begin{equation}
\det ( \mat{A} + \mat{B} ) = \det ( \mat{B} + \mat{A} ),
\end{equation}
but matrix addition is not commutative with calculating determinants, so
\begin{equation}
\det ( \mat{A} + \mat{B} ) \neq \det \mat{A} \det \mat{B}.
\end{equation}
\item The determinant of any triangular matrix (upper/lower triangular and diagonal matrices) are just the product of the main diagonal.
\item An alternative way to calculate the determinant is based on the trace of a matrix using the Cayley-Hamilton theorem\footnote{\href{https://en.wikipedia.org/wiki/Cayley\%E2\%80\%93Hamilton_theorem}{Cayley–Hamilton theorem on Wikipedia.}} for a $2 \times 2$ matrix $\mat{M}_{2}$,
\begin{equation}
\det \mat{M}_{2} = \frac{ (\tr \mat{M}_{2})^{2} - \tr \mat{M}_{2}^{2} }{2},
\end{equation}
and for a $3 \times 3$ matrix $\mat{M}_{3}$,
\begin{equation}
\det \mat{M}_{3} = \frac{ (\tr \mat{M}_{3})^3 - 3\tr \mat{M}_{3}^2 \tr \mat{M}_{3} + 2 \tr \mat{M}^3 }{6}
\end{equation} 
\end{enumerate}

\exercise{2(a-c)}

Let $\mat{E}$ be an elementary matrix produced by one of the following operations. 
\begin{enumerate}
\item If $\mat{E}$ is produced by scalar $k$ in a row-scaling operation, $\det \mat{E} = k.$
\item If $\mat{E}$ is produced by a scalar addition, $\det \mat{E} = 1$ (unchanged).
\item If $\mat{E}$ is produced by a row swap, $\det \mat{E} = -1$.
\end{enumerate}

\theorem{12}{If $\det \mat{A} \neq 0$ then $\mat{A}$ is invertible.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$. If $\det \mat{A} \neq 0$ then $\mat{A}$ can be reduced to $\mat{I}_{n}$ using only elementary operations, implying $\mat{A}$ satisfies $\mat{EA} = \mat{I}$ for some composite/product matrix of elementary matrices, $\mat{E}$. By definition, $\mat{E} = \mat{A}^{-1}$, making $\mat{A}$ invertible.
\end{proof}

\theorem{13}{If $\mat{A}$ is invertible then $\det \mat{A} \neq 0$.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be an invertible matrix. $\mat{A}$ therefore satisfies $\mat{EA} = \mat{I}$ for some matrix $\mat{E}$ comprised of the product of elementary matrices. By definition, $\mat{A} = \mat{E}^{-1}$, and as the inverse of the product of elementary matrices will be the product of elementary matrices, $\mat{A}$ is also a product of elementary matrices. As the determinant of any elementary matrix is nonzero, and the determinant of a product is the product of determinants, $\det \mat{A} = \det \mat{E}^{-1} \neq 0$. 
\end{proof}

\theorem{14}{If $\mat{A}$ and $\mat{B}$ are invertible matrices of the same size, then $\mat{A} + \mat{B}$ is invertible.}

\begin{proof}
This is incorrect, because $\det \mat{A} + \det \mat{B} \neq \det ( \mat{A} + \mat{B} )$. For example, if $\mat{A} = -\mat{B}$. Let $\mat{A} = -\mat{I}_{2}$ and $\mat{B} = \mat{I}_{2}$. $\det \mat{A} = \det \mat{B} = 1$, but $\det ( \mat{A} + \mat{B} ) = 0$.
\end{proof}

\theorem{15}{If $\mat{A}$ is a square matrix, then $\det \mat{A} = \det \mat{A}^{T}.$}

\begin{proof}
Let $\mat{M} \in \mathbb{R}^{n \times n}. \det \mat{M}$ is defined via permutations of row and column indices of $\mat{M}$, so swapping the rows and columns will not affect $\det \mat{M}$.
\end{proof}

\theorem{16}{If $\mat{A}$ and $\mat{B}$ are matrices of the same size then $\mat{A}$ and $\mat{B}$ are invertible if and only if $\mat{AB}$ is invertible.}

\begin{proof}
Let $\mat{A}, \mat{B} \in \mathbb{R}^{n \times n}$. If $\mat{AB}$ is invertible, then $\det \mat{AB} \neq 0$, and as $\det \mat{AB} = \det \mat{A} \det \mat{B}$, $\det \mat{A} \neq 0$ and $\det \mat{B} \neq 0$. Therefore, $\mat{A}$ and $\mat{B}$ are invertible. Opposingly, if $\mat{A}$ and $\mat{B}$ are invertible, $\det \mat{A} \ne 0$ and $\det \mat{B} \ne 0$. Therefore, $\det \mat{AB} \ne 0$, so $\mat{AB}$ must be invertible.
\end{proof}

\subsection{Lab 6: 4 $\times$ 4 Determinants and Beyond}

\notes

\begin{enumerate}
\item The equation for calculating matrices higher than $2 \times 2$ follows a row or column-based expansion in form
\begin{equation}
\det \mat{M} \bigg\rvert_{i=c} = \sum^{n}_{j = 1} (-1)^{c+j} a_{cj} \det( \mat{M}^{*}_{cj} \in \mat{M} )
\end{equation}
for a row-based expansion at row $c$ or
\begin{equation}
\det \mat{M} \bigg\rvert_{j=c} = \sum^{n}_{i = 1} (-1)^{i+c} a_{ic} \det( \mat{M}^{*}_{ic} \in \mat{M} )
\end{equation}
for a column-based expansion, given matrix $\mat{M} \in \mathbb{R}^{n \times n}$, and $\mat{M}^{*}_{ij}$ denotes a submatrix anchored by $(i,j)$, meaning that row $i$ and column $j$ are removed from $\mat{M}$ to produce $\mat{M}^{*}_{ij}$. 

For example, for a $3 \times 3$ matrix $\mat{A}$,
\begin{align*}
\det \mat{A} \bigg\rvert_{i=1} 
&= \sum_{j = 1}^{3} (-1)^{1+j} a_{1j} \det ( \mat{A}^{*}_{1j} \in \mat{A} ) \\
&= a_{11} \det \begin{pmatrix}
  a_{22} & a_{23} \\
  a_{32} & a_{33}
\end{pmatrix} -
a_{12} \det \begin{pmatrix}
  a_{21} & a_{23} \\
  a_{31} & a_{33}
\end{pmatrix} +
a_{13} \det \begin{pmatrix}
  a_{21} & a_{22} \\
  a_{31} & a_{32}
\end{pmatrix}
\end{align*}
\end{enumerate}

\newpage
\exercise{1(a)}

\begin{align*}
M_{41} &= \det \begin{pmatrix}
  1 & 0 & 0 \\
  2 & 1 & 0 \\
  1 & 3 & 1
\end{pmatrix}
= \det \begin{pmatrix}
  1 & 0 \\
  3 & 1
\end{pmatrix}
 = 1 \\
M_{42} &= \det \begin{pmatrix}
  1 & 0 & 0 \\
  1 & 1 & 0 \\
  2 & 3 & 1
\end{pmatrix}
 = \det \begin{pmatrix}
   1 & 0 \\
   3 & 1
\end{pmatrix}
= 1 \\
M_{43} &= \det \begin{pmatrix}
  1 & 1 & 0 \\
  1 & 2 & 0 \\
  2 & 1 & 1
\end{pmatrix}
= \det \begin{pmatrix}
  2 & 0 \\
  1 & 1
\end{pmatrix} -
\det \begin{pmatrix}
  1 & 0 \\
  2 & 1
\end{pmatrix}
= 2 - 1 = 1 \\
M_{44} &= \det \begin{pmatrix}
  1 & 1 & 0 \\
  1 & 2 & 1 \\
  2 & 1 & 3
\end{pmatrix}
= \det \begin{pmatrix}
  2 & 1 \\
  1 & 3
\end{pmatrix}
- \det \begin{pmatrix}
  1 & 1 \\
  2 & 3
\end{pmatrix}
= 6 - 1 - (3 - 2) = 4
\end{align*}

\exercise{2(b)}

\begin{align*}
\det \mat{A} \bigg\rvert_{i=4} &= \sum_{j=1}^{4} (-1)^{4 + j} A_{4j} M_{4j} \\
&= -A_{41} M_{41} + A_{42} M_{42} - A_{43} M_{43} + A_{44} M_{44} \\
&= 4M_{44} - M_{43} = 4(4) - 1 = 15 
\end{align*}

\exercise{2(c)}

\begin{align*}
\det \mat{A} \bigg\rvert_{i=1} &= \sum^{4}_{j=1} (-1)^{1+j} A_{1j} M_{1j} \\
&= M_{11} - M_{12} 
= \det \begin{pmatrix} 2 & 1 & 0 \\ 1 & 3 & 1 \\ 0 & 1 & 4  \end{pmatrix}
- \det \begin{pmatrix} 1 & 1 & 0 \\ 2 & 3 & 1 \\ 0 & 1 & 4  \end{pmatrix} \\
&= 2 \det \begin{pmatrix} 3 & 1 \\ 1 & 4 \end{pmatrix} 
- \det \begin{pmatrix} 1 & 1 \\ 0 & 4 \end{pmatrix} 
- \det \begin{pmatrix} 3 & 1 \\ 1 & 4 \end{pmatrix}
+ \det \begin{pmatrix} 2 & 1 \\ 0 & 4  \end{pmatrix} \\
&= 2 (11) - 4 - (11) + 8 = 15.
\end{align*}

\exercise{2(d)}

\begin{align*}
\det \mat{B} &= \det \begin{pmatrix} 1 & 1 & 0 \\ -1 & 3 & 1 \\ 0 & 1 & 4 \end{pmatrix} 
= \det \begin{pmatrix} 3 & 1 \\ 1 & 4 \end{pmatrix}
- \det \begin{pmatrix} -1 & 1 \\ 0 & 4 \end{pmatrix} \\
&= 11 - (-4) = 15
\end{align*}

Because $\mat{P}$ is triangular,
\begin{align*}
\det \mat{P} = \prod_{i=1}^{n} P_{ii} = -15
\end{align*}

\exercise{2(e)}

\begin{equation*}
\mat{E}_{1} = \begin{pmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & -4 \\
  0 & 0 & 0 & 1
\end{pmatrix} 
\hspace{3em}
\mat{E}_{2} = \begin{pmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 1 & 0 \\
  0 & 0 & 0 & 1
\end{pmatrix} 
\hspace{3em}
\mat{E}_{3} = \begin{pmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 1 & 0
\end{pmatrix}
\end{equation*}

\exercise{2(f)}

As $\det \mat{AB} = \det \mat{A} \det \mat{B}$, then
\begin{equation*}
\det \mat{B} = \frac{ \det \mat{P} }{ \det \mat{E}_{1} \det \mat{E}_{2} \det \mat{E}_{3} } = \frac{15}{ (1)(1)(-1) } = -15
\end{equation*}

\section{Chapter 2: Invertibility}

\subsection{Lab 7: Singularity}

\notes

\begin{enumerate}
\item Given a linear system $\mat{A}x = \mat{B}$, if $\mat{A}$ is invertible, then the system has exactly one solution: $x = \mat{A}^{-1} \mat{B}$.
\item If the constants in a linear system ($\mat{Ax = B}$) are all zero (i.e., $\mat{B = 0}$), we call the linear system a homogenous linear system, and these systems always have at least one solution: $\mat{x = 0}$.
\end{enumerate}

\exercise{2(a)}

$\mat{A}$ is invertible because $\det \mat{A} \neq 0$. As
$$\mat{A}^{-1} = 
\begin{pmatrix} 
  0 & -2 & 1 \\
  \frac{1}{2} & 1 & -\frac{1}{2} \\
  -\frac{1}{6} & 0 & \frac{1}{6}
\end{pmatrix},$$
then
$$\mat{x} = \mat{A}^{-1}\mat{B} =
\begin{pmatrix}
-20 \\ 27/2 \\ -7/6
\end{pmatrix}.$$

\exercise{2(b)}

Likewise, for $\mat{Ax} = \mat{0}, \mat{x} = \mat{0}$, the trivial solution.

\theorem{17}{The inverse of a nonsingular upper triangular matrix is upper triangular.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be a nonsingular upper triangular matrix. Since $\mat{A}$ is invertible, there exists a matrix $\mat{A}^{-1}$ such that $\mat{A} \mat{A}^{-1} = \mat{I}$.

Recall that the product of two upper triangular matrices is upper triangular. Moreover, the inverse of a nonsingular matrix is a product of elementary matrices. Among the elementary matrices, only those corresponding to row operations that preserve upper triangular form (scaling a row, adding a multiple of a row to another row above it, or swapping rows above the diagonal, which are disallowed in upper triangular matrices anyway) are involved in forming an upper triangular matrix.

Since $\mat{A}$ is upper triangular, its Gaussian elimination to the identity matrix involves only upper triangular-preserving row operations. Therefore, the inverse $\mat{A}^{-1}$, being the product of the inverses of those elementary matrices, also consists of upper triangular matrices.

Since a product of upper triangular matrices is upper triangular, it follows that $\mat{A}^{-1}$ is upper triangular.
\end{proof}

\theorem{18}{The inverse of a nonsingular diagonal matrix is diagonal.}

\begin{proof}
Let $\mat{A}$ be an invertible/nonsingular diagonal matrix. Therefore, $\mat{A} \in \mathbb{R}^{n \times n}$ and $\mat{A}$ can be expressed as the product of elementary matrices, $\mat{E}$. As $\mat{I}$ is diagonal, the only elementary operations that can occur within $\mat{E}$ must maintain the diagonal property (with any operations that violate this property at any point being later undone). Similarly, $\mat{A}^{-1} = \mat{E}^{-1}$, and as the inverse of an elementary product is an elementary product of the inverse operations, the diagonal property must be maintained through $\mat{E}^{-1}$ as well.   
\end{proof}

\theorem{19}{The determinant of the inverse is equal to the multiplicative inverse of the determinant, or $\det \mat{A}^{-1} = (\det \mat{A})^{-1}$.}

\begin{proof}
We use the multiplicative property of the determinant: for any \( n \times n \) matrices \( \mat{A} \) and \( \mat{B} \),
\[
\det(\mat{A} \mat{B}) = \det(\mat{A}) \cdot \det(\mat{B}).
\]

Suppose \( \mat{A} \) is an invertible \( n \times n \) matrix. Then \( \mat{A}^{-1} \mat{A} = \mat{I} \), where \( \mat{I} \) is the identity matrix. Taking the determinant of both sides, we get:
\[
\det(\mat{A}^{-1} \mat{A}) = \det(\mat{I}) = 1.
\]

Using the multiplicative property:
\[
\det(\mat{A}^{-1}) \cdot \det(\mat{A}) = 1.
\]

Solving for \( \det(\mat{A}^{-1}) \), we obtain:
\[
\det(\mat{A}^{-1}) = \frac{1}{\det(\mat{A})}.
\]

\end{proof}

\theorem{20}{$\mat{A}$ is invertible if and only if $\mat{A}$ can be written as a product of elementary matrices.}

\begin{proof}
For $\mat{A}$ to be invertible, there must exist some matrix $\mat{E}$ as the product of elementary matrices such that $\mat{EA = I}$, with $\mat{E} = \mat{A}^{-1}$. Therefore, $\mat{A}^{-1}$ is a product of elementary matrices, and as the inverse of an elementary matrix is elementary, the inverse of the product of elementary matrices is also the product of elementary matrices. Therefore, as $\mat{A} = \mat{E}^{-1}$, $\mat{A}$ can be expressed as the prodct of elementary matrices.
\end{proof}

\theorem{21}{If $\mat{A}$ is an $n \times n$ invertible matrix, then the system $\mat{Ax} = \mat{B}$ has exactly one solution for all $n \times 1$ vectors $\mat{B}$.}

\begin{proof}
By definition, for $\mat{Ax=B}$ to have $\ge 1$ solution, $\mat{A}$ must be either underrepresented ($n \times m$) or contradicting ($\det \mat{A} = 0$). Let $\mat{A} \in \mathbb{R}^{n \times n}$ be invertible. Therefore, $\mat{A}$ cannot contradict (meaning that, when represented as linear equations, the equations either state the same information or directly state conflicting information) as $\det \mat{A} \ne 0$. Additionally, as $\mat{A}$ is $n \times n$, $\mat{A}$ cannot be underrepresented: all values can be calculated. Therefore, $\mat{x}$ must have exactly one solution.
\end{proof}

\theorem{22}{If $\mat{A}$ is an $n \times n$ matrix and the system $\mat{Ax} = \mat{B}$ is consistent (has at least one solution) for all $n \times 1$ vectors $\mat{B}$, then $\mat{A}$ is invertible.}

\begin{proof}
As $\mat{Ax=B}$ is consistent, it cannot contain a contradiction (two of the internal linear equations stating conflicting information), because if there was a contradiction, $\mat{Ax=B}$ would yield no solutions and $\det \mat{A} = 0$, making $\mat{A}$ singular. Therefore, $\mat{A}$ must be invertible to have $\ge 1$ solution to $\mat{Ax = B}$.  
\end{proof}

\theorem{23}{If $ad - bc \neq 0$ then $\begin{pmatrix} a & b \\ c & d \end{pmatrix}^{-1} = \frac{1}{(ad - bc)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$.}

\begin{proof}
    This can be verified by direct calculation. Let $\mat{A} = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$. By gaussian elimination:
\begin{align*}
  (\mat{A|I})
= \begin{pmatrix} a & b & 1 & 0 \\ c & d & 0 & 1 \end{pmatrix}
%
&\xrightarrow{R_2 \leftarrow R_2 - \frac{c}{a} R_1}
\begin{pmatrix}
a & b & 1 & 0 \\
0 & d - \frac{cb}{a} & -\frac{c}{a} & 1
\end{pmatrix} \\
%
&\xrightarrow{\substack{R_1 \leftarrow \frac{1}{a} R_1 \\ R_2 \leftarrow \frac{a}{ad - bc} R_2}}
\begin{pmatrix}
1 & \frac{b}{a} & \frac{1}{a} & 0 \\
0 & 1 & \frac{-c}{ad - bc} & \frac{a}{ad - bc}
\end{pmatrix} \\
%
&\xrightarrow{R_1 \leftarrow R_1 - \frac{b}{a} R_2}
\begin{pmatrix}
1 & 0 & \frac{d}{ad - bc} & \frac{-b}{ad - bc} \\
0 & 1 & \frac{-c}{ad - bc} & \frac{a}{ad - bc}
\end{pmatrix}.
\end{align*}
Therefore,
\begin{equation*}
\mat{A}^{-1} =
\begin{pmatrix}
\frac{d}{ad - bc} & \frac{-b}{ad - bc} \\
\frac{-c}{ad - bc} & \frac{a}{ad - bc}
\end{pmatrix}
= \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\end{equation*}
\end{proof}

\theorem{24}{$\mat{A}$ is an $n \times n$ invertible matrix if and only if the system $\mat{Ax} = \mat{0}$ has only trivial solution.}

\begin{proof}
Suppose \( \mat{A} \in \mathbb{R}^{n \times n} \) is invertible. Then, by Theorem 21, for any \( \vec{b} \in \mathbb{R}^n \), the equation \( \mat{Ax} = \vec{b} \) has a unique solution. In particular, taking \( \vec{b} = \vec{0} \), the homogeneous equation \( \mat{Ax} = \vec{0} \) must also have a unique solution. Since \( \vec{x} = \vec{0} \) is always a solution to \( \mat{Ax} = \vec{0} \), it must be the only solution — the solution is trivial.

Conversely, suppose the equation \( \mat{Ax} = \vec{0} \) has only the trivial solution \( \vec{x} = \vec{0} \). Then the null space of \( \mat{A} \) is trivial, so the columns of \( \mat{A} \) are linearly independent. For an \( n \times n \) matrix, linear independence of columns implies that \( \mat{A} \) has full rank \( n \), and hence is invertible.

Therefore, \( \mat{A} \) is invertible if and only if the homogeneous equation \( \mat{Ax} = \vec{0} \) has only the trivial solution.
\end{proof}

\subsection{Lab 8: Modularity and $Z_{p}$}

\notes

\begin{enumerate}
  \item If $x$ and $y$ are integers, they are said to be congruent modulo $p$ (written $x \equiv y \pmod{p}$), meaning that $\frac{x - y}{p}$ is an integer.
\item $r = n \pmod{d}$ can be interpreted as the integer remainder of $n/d$ where $n, d$ and $r$ are all integers.
\item $Z_{p}$ is the set of all integers that can result from $\pmod{p}$ (not considering congruency), and for integer value $p$, this set is all integers $Z_{p} = \{ 0, 1, \dots, p-2, p-1 \}$. 
\end{enumerate}

\exercise{1(a)}

Given $Z_{5} = \{0, 1, 2, 3, 4 \}$, the corresponding additive inverses would be $(0, 4, 3, 2, 1)$ (as each indexwise matchup would thereby result in a modular congruency with 0, such as $1 + 4 \equiv 0 \pmod{5}$).

\exercise{1(b)}

The corresponding multiplicative inverses for $x \ge 0 \in Z_{5}$ would be $(\textit{undefined}, 1, 3, 2, 4)$, as, for example, the integer remainder of $\frac{4 \times 4}{5}$ is 1. 

\exercise{2(a)}

Let $\mat{A} = \begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix}. \mat{A} \equiv \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \pmod{2},$ thus $\det \mat{A} \pmod{2} \neq 0$, therefore $\mat{A}$ is invertible modulo 2. Similarly, $\mat{A} \pmod{5} = \mat{A}$ as all entries of $\mat{A} < 5$. Therefore, as $\det \mat{A} = \det \mat{A} \pmod{5} \ne 0, \mat{A}$ is invertible modulo 5.

\exercise{2(b)}

Let $\mat{A} = \begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix}$ and $\mat{B} = \begin{pmatrix} 4 \\ 1 \end{pmatrix} \pmod{2}.$ $\mat{A} \equiv \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \pmod{2},$ so $\mat{A}^{-1} = \begin{pmatrix} 1 & 0 \\ -1 & 1 \end{pmatrix} \pmod{2}$, therefore
\begin{equation*}
\mat{x}\pmod{2} = \mat{A}^{-1} \mat{B}\pmod{2} = \begin{pmatrix} 4 \\ -3 \end{pmatrix} \pmod{2}.
\end{equation*}
Similarly, the calculation modulo 5 is just the standard calculation (as all values are $< 5$), so
\begin{equation*}
\mat{x}\pmod{5} = \mat{A}^{-1} \mat{B} = \begin{pmatrix} -10 \\ 17 \end{pmatrix} \equiv \begin{pmatrix} 0 \\ 2 \end{pmatrix} \pmod{5}.
\end{equation*}

\subsection{Lab 9: Complex Entries and Eigenvalues}

\notes

\begin{enumerate}
\item Where a complex number is defined with a real and imaginary part in form $a + bi$, its complex conjugate will have the imaginary coefficient as the multiplicative inverse, $a - bi$.
\item The complex conjugate of a matrix with complex entries has all of the entries of the matrix as the complex conjugates of the original matrix's entires.
\item For an $n \times n$ matrix $\mat{A}$, the eigenvalues $\lambda$ of $\mat{A}$ are values of $\lambda$ which satisfy 
\begin{equation}
\mat{Ax} = \lambda \mat{x}
\end{equation}
for each eigenvalue's corresponding eigenvector $\mat{x}$, and the characteristic equation,
\begin{equation}
\det ( \mat{A} - \lambda \mat{I} ) = 0.
\end{equation}
\begin{enumerate}
\item The eigenvalues of a diagonal matrix are just the entries along the diagonal.
\item Singular matrices always have 0 as an eigenvalue, at least (they may have more).
\item The eigenvalues of a matrix $\mat{M}$ are the same as the eigenvalues of its transposition $\mat{M}^{T}$:
\begin{equation}
\eig \mat{M} = \eig \mat{M}^{T}
\end{equation}
\item The eigenvalues of a complex-valued matrix $\mat{M}$ are the same as the eigenvalues of its complex transposition $\overline{\mat{M}}^{T}$:
\begin{equation}
\eig \mat{M} = \eig \mat{\overline{M}}^{T}
\end{equation}
\end{enumerate}
\item A matrix $\mat{M}$ is considered ``Hermetian'' if it is equal to its complex transpose: $\mat{M} = \mat{\overline{M}}^{T}$.
\item A matrix is considered ``Unitary'' if its complex transpose is its inverse: $\mat{M}^{-1} = \mat{\overline{M}}^{T}$.
\end{enumerate}

\exercise{2(a)}

Let $\mat{A} = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}$. $\eig \mat{A} = \{ \lambda \}$ where
\begin{align*}
\det(\mat{A} - \lambda \mat{I}) 
&= \det \begin{pmatrix} 2 - \lambda & 0 \\ 0 & 3 - \lambda \end{pmatrix} \\
&= (2-\lambda)(3-\lambda) = 0 \\
\end{align*}
$\therefore \eig \mat{A} = \{ 2,3 \}$, which is the values on the diagonal.

\exercise{2(b)}

\conjecture{The eigenvalues of any diagonal matrix are the values along the main diagonal.}

\begin{proof}
The eigenvalues must be the values along the main diagonal, as if any of these values were to become 0, the matrix would become singular. This is because of the telescoping property of the determinant, in that for any diagonal matrix $\mat{M} \in \mathbb{R}^{n \times n}$,
\begin{equation*}
\det \mat{M} = \prod_{i = 0}^{n} \mat{M}_{ii},
\end{equation*}
thus if any of these values is 0, $\det \mat{M} = 0$, breaking invertibility.
\end{proof} 

\exercise{2(c)}

Let $\mat{A} = \begin{pmatrix} 1 & 4 \\ 2 & 3 \end{pmatrix}$.
\begin{align*}
\det (\mat{A} - \lambda \mat{I})
&= \det \begin{pmatrix} 1 - \lambda & 4 \\ 2 & 3 - \lambda \end{pmatrix} \\
&= (1 - \lambda)(3 - \lambda) - 8 \\
&= \dots \\
&= (\lambda - 5)(\lambda + 1) = 0 \\
\det (\mat{A}^{T} - \lambda \mat{I})
&= \det \begin{pmatrix} 1 - \lambda & 2 \\ 4 & 3 - \lambda \end{pmatrix} \\
&= \dots \\
&= (\lambda - 5)(\lambda + 1) = 0
\end{align*}
$\therefore \eig \mat{A} = \eig \mat{A}^{T} = \{ -1, 5 \}$.

\exercise{2(d)}

As
\begin{equation*}
\mat{A} = \begin{pmatrix} 1 & -i \\ 2i & i \end{pmatrix}, \hspace{2em} \text{ and } \hspace{2em} \ct{A} = \begin{pmatrix} 1 & -2i \\ i & -i \end{pmatrix},
\end{equation*}
As $\eig \mat{A} = \{\lambda: \det(\mat{A} - \lambda \mat{I}) = 0 \}$, $\eig \mat{A} = \eig \ct{A} = \{ \frac{1 - i \pm \sqrt{8 - 6i} }{2}  \}$. 

This is similar to the properties seen in example 1(c), as the transposition alters the order of elements in such a way that for real numbers, this makes no change to the determinant, but for complex values, the sign will be altered ($\det \mat{A} = \det \mat{A}^{T}$ holds true for the matrix of real values $\mat{A}$ but not for a complex-valued matrix). Therefore, the complex conjugate needs to be taken to fix alter the signs such that it's once again equivalent.

\exercise{2(e)}

An example of a hermitian matrix with complex entries would be
\begin{equation*}
\mat{H} = \begin{pmatrix} 1 & 2-i \\ 2+i & 1 \end{pmatrix}.
\end{equation*}

\exercise{2(f)}

\conjecture{Where $\mat{A}$ is Hermitian, $\eig \mat{A} = \eig \ct{A}$.}

\begin{proof}
$\mat{A}$ is Hermitian, so $\mat{A} = \ct{A}$.
\end{proof}

\exercise{2(g)}

An example of a unitary matrix is
\begin{equation*}
\mat{U} = \frac{1}{\sqrt{2}}  \begin{pmatrix} i & -1 \\ 1 & -i \end{pmatrix}.
\end{equation*}

\exercise{2(h)}

\conjecture{The eigenvalues of the complex transpose of a unitary matrix are the inverses of the eigenvalues of the matrix ($\eig \mat{U} = (\eig \mat{U}^{-1})^{-1}$).}

\begin{proof}
As $\det \mat{A}^{-1} = (\det \mat{A})^{-1}$ for any $\mat{A} \in \mathbb{R}^{n \times n}$, $\eig \mat{A}^{-1} = (\eig \mat{A})^{-1}$. Therefore, for any unitary matrix $\mat{U} \in \mathbb{R}^{n \times n}$, as $\mat{U}^{-1} = \ct{U}$, $\eig \mat{U} = (\eig \ct{U})^{-1}$.
\end{proof}

\theorem{25}{Determinants and complex conjugates are commutative, or the determinant of the complex conjugate is equal to the complex conjugate of the determinant. I.E.,
\begin{equation*}
\det \overline{\mat{A}} = \overline{\det \mat{A}}
\end{equation*}.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}.$ $\det \mat{A}$ is expressed as a combination of the values of $\mat{A}$ via multiplication and division in form
\begin{equation*}
\det \mat{A} = a_{11} ( a_{22} \det (\dots) - a_{23} \det (\dots) + \dots ) - a_{12} ( a_{21} \det(\dots) - \dots ) + \dots,
\end{equation*}
and as taking the complex conjugate is commutative with addition and multiplication (and thus subtraction and division)
\begin{equation*}
\overline{ab} = \overline{a} \times \overline{b} \hspace{2em} \overline{a+b} = \overline{a} + \overline{b},
\end{equation*}
taking the complex conjugate of the determinant is equivalent to calculating the determinant with the complex conjugate of each of the internal values: 
\begin{equation*}
\overline{\det \mat{A}} = \overline{a_{11}} ( \overline{a_{22}} \det (\dots) - \overline{a_{23}} \det (\dots) + \dots ) - \overline{a_{12}} ( \overline{a_{21}} \det(\dots) - \dots ) + \dots,
\end{equation*}
which would be the same as taking the determinant of the complex conjugate of $\mat{A}$, or
\begin{equation*}
\overline{\det \mat{A}} = \det \overline{\mat{A}}.
\end{equation*}
\end{proof}

\theorem{26}{If $\mat{A}$ is invertible, then inversion is commutative with the complex conjugate. I.E., 
\begin{equation*}
\left(\overline{\mat{A}}\right)^{-1} = \overline{\mat{A}^{-1}}
\end{equation*}}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be an invertible matrix, implying that $\mat{A}$ can be expressed as the product of elementary matrices $\mat{E}$. Similarly, $\mat{A}^{-1}$ is expressed as the $\mat{E}^{-1}$, the the product of the inverse elementary operations comprising $\mat{A}$, and because the values that comprise $\mat{A}^{-1}$ can be expressed as the inverses of the elementary operations comprising $\mat{A}$, the values of $\mat{A}^{-1}$ can be expressed as combinations of the elements of $\mat{A}$ using basic operations: addition, subtraction, multiplication, and division. Because taking the complex conjugate applied before or after any basic operation yields the same result, taking the complex conjugate before or after taking the inverse will yield the same value. 
\end{proof}

\theorem{27}{If $c$ is a complex number, $\overline{c\mat{A}} = c\overline{\mat{A}}$.}

\begin{proof}
This is untrue as for any complex scalar $c$ and matrix $\mat{A} \in \mathbb{R}^{m \times n}$, because as scalar multiplication is distributed across all elements, and as for any two complex-valued numbers $a$ and $b$, $\overline{ab} \ne a \overline{b}$ Therefore, $\overline{c} \times \overline{\mat{M}}$
\end{proof}

\theorem{28}{The eigenvalues of a diagonal matrix are the entries on the diagonal.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be a diagonal matrix.
\begin{equation*}
\det \mat{A} = \prod_{i=1}^{n} A_{ii}.
\end{equation*}
Thus, if any value in $\{A_{11}, \dots, A_{nn}\}$ is 0, $\det \mat{A} = 0$, breaking invertbility. Therefore, all values of the main diagonal are eigenvalues, as all values satisfy
\begin{equation*}
\det( \mat{A} - \lambda \mat{I} ) = 0
\end{equation*} 
\end{proof}

\theorem{29}{All eigenvalues of Hermitian matrices are real numbers.}

% SHAMELESSLY COPIED FROM CHATGPT!
%
% I genuinely did not know how to prove this.

\begin{proof}
Let \( \mat{H} \in \mathbb{C}^{n \times n} \) be a Hermitian matrix, so \( \mat{H} = \mat{H}^\dagger \), meaning \( \mat{H} \) equals its conjugate transpose.

Suppose \( \lambda \in \mathbb{C} \) is an eigenvalue of \( \mat{H} \) with corresponding (nonzero) eigenvector \( \va{x} \in \mathbb{C}^n \), so:
\[
\mat{H} \va{x} = \lambda \va{x}.
\]

Take the Hermitian inner product of both sides with \( \vec{x} \):
\[
\ct{x} \mat{H} \va{x} = \ct{x} (\lambda \va{x}) = \lambda \ct{x} \va{x}.
\]

Now, note two facts:
\begin{enumerate}
    \item \( \ct{x} \mat{H} \va{x} \) is a \textbf{real number}, because \( \mat{H} \) is Hermitian.
    \item \( \ct{x} \va{x} \) is real and positive, since \( \va{x} \ne 0 \).
\end{enumerate}

Therefore, \( \lambda \ct{x} \va{x} \) is real, which implies that \( \lambda \) must be real.
\end{proof}

\theorem{30}{The complex conjugate of a Hermitian matrix is a Hermitian matrix.}

\begin{proof}
Let $\mat{H} \in \mathbb{R}^{n \times n}$ be a hermitian matrix. Therefore, $\mat{H} = \ct{H}$.
\end{proof}

\theorem{31}{$\mat{A}$ is Unitary if and only if $\mat{A}^{-1} = \ct{A}$}

\begin{proof}
This is true by the definition of a unitary matrix, as to be unitary, $\mat{M} \ct{M} = \ct{M} \mat{M} = \mat{I}$, and similarly, $\mat{M} \mat{M}^{-1} = \mat{M}^{-1} \mat{M} = \mat{I}$, therefore, $\mat{M}^{-1} = \ct{M}$ by the associative property.
\end{proof}

\theorem{32}{$\lambda$ is an eigenvalue of matrix $\mat{A}$ if and only if it is an eigenvalue of $\ct{A}$.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$. As $\det \mat{A} = \det \ct{A}$, then therefore, $\det(\mat{A} - \lambda \mat{I}) = \det(\ct{A} - \lambda \mat{I})$ as $\ct{\mat{A} - \lambda \mat{I}} = \ct{A} - \overline{\lambda} \mat{I}$. Therefore, as $\eig \mat{A} = \{ \lambda | \det(\mat{A} - \lambda \mat{I}) = 0 \}, \eig \mat{A} = \eig \ct{A}$ and vice versa.
\end{proof}

\subsection{Lab 10: Linear Combinations and Dependency}

\notes

\begin{enumerate}
\item If $S = \{ \mat{v}_{1}, \mat{v}_{2}, \dots, \mat{v}_{n} \}$ is a set of vectors (or perhaps, equivalently, a vector of vectors, making $\mat{S}$ a matrix) and there exists scalars within some vector $\mat{k} = (k_{1}, k_{2}, \dots, k_{n})$ such that
\begin{equation}
\mat{w} = \sum_{i=1}^{n} k_{i} \mat{v}_{i} = \mat{Sk},
\end{equation}
then we say that $\mat{w}$ is a linear combination of all $\mat{v} \in \mat{S}$.
\item For set $\mat{S} = \{ \mat{v}_{1}, \dots, \mat{v}_{n} \}$, $\mat{S}$ is linearly independent if
\begin{equation}
\mat{0} = \sum_{i=1}^{n} k_{i} \mat{v}_{i} = \mat{Sk}
\end{equation}
has only the trivial solution, $\mat{k = 0}$. Otherwise, it's linearly dependent, meaning that some nonzero value of $\mat{k}$ can result in $\mat{w=0}$.
\item A set of vectors $\mat{S}$ is said to span a vector space $\mat{V}$ if every vector in $\mat{V}$ can be expressed as a linear combination of all of the vectors in $\mat{S}$.
\begin{enumerate}
\item A common example is dimensionality, like $\mathbb{R}^{1}, \mathbb{R}^{2}, \mathbb{R}^{3}, \mathbb{R}^{n}, \mathbb{R}^{n \times n}, \dots$. 
\item The set $\mat{S}_{1} = \{1\}$ spans $\mathbb{R}^{1}$ as all numbers can be written as a product with 1.
\item Similarly, $\mat{S}_{2} = \{(1,0), (0,1)\}$ spans $\mathbb{R}^{2}$ because all vectors within two dimensions can be described as a linear combination of the two.
\end{enumerate}
\end{enumerate}

\theorem{33}{If $\mat{A}$ is invertible, then the rows of $\mat{A}$ are linearly independent.}

\begin{proof}
For $\mat{A}$ to be invertible, it must satisfy $\mat{EA=I}$ for some product of elementary matrices $\mat{E}$ where $\mat{E}=\mat{A}^{-1}$, allowing $\mat{A}$ to reduce down to $\mat{I}$. As the rows of $\mat{I}$ are clearly linearly independent and all valid elementary operations (scalar multiplication, scalar addition, and swapping) can only produce linearly independent matrices, all elementary matrices and subsequent elementary products must be likewise linearly independent, including $\mat{A}$, which must be expressible as a product of elementary operations.    
\end{proof}

\theorem{34}{If $\mat{A}$ is invertible, then the columns of $\mat{A}$ are linearly independent.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$. Suppose $\mat{A}$ has linearly dependent columns in form such that for some column vector $\mat{k} \neq \mat{0}$, $\mat{Ak} = \mat{0}$. To be invertible, $\mat{A}$ must satisfy
$$\mat{A}^{-1}\mat{A} = \mat{I},$$
so if we multiply by $\mat{k}$ on both sides, knowing that $\mat{Ik}=\mat{k}$, we obtain
$$\mat{A}^{-1} \mat{0} = \mat{k}$$
or
$$\mat{k} = \mat{0},$$
a contradiction. Therefore, to be invertible, the columns of $\mat{A}$ must be linearly independent.
\end{proof}

\theorem{35}{A set of vectors with only two vectors in it is linearly dependent if one is a scalar multiple of the other.}

\begin{proof}
True by definition. Let $S = \{ \mat{x}, s\mat{x} \}$. For $\mat{k} = ( -s, 1 ), -s\mat{x}+s\mat{x} = \mat{0}$.
\end{proof}

\theorem{36}{A set of vectors is linearly dependent if it contains the zero vector.}

\begin{proof}
This is not true. For example, consider $S=\{ (0,0), (0,1), (1,0) \}$. Despite containing the zero vector $\mat{0}=(0,0)$, there is no $\mat{k}$ such that $\mat{Sk=0}$ where $\mat{k} \ne \mat{0}$.
\end{proof}

\theorem{37 and 38}{If $\mat{A}$ is an $n \times n$ invertible matrix, then the rows and columns of $\mat{A}$ span $\mathbb{R}^{n}$.}

\begin{proof}
As shown, for $\mat{A}$ to be invertible, the set of vectors representing its rows or its columns must be each linearly independent, containing exactly $n$ vectors of size $n$. Therefore, as any set of $n$ linearly independent vectors of size $n$ can span $\mathbb{R}^{n}$, the rows and columns of $\mat{A}$ span $\mathbb{R}^{n}$.    
\end{proof}

\subtheorem{Any set of $n$ linearly independent vectors of size $n$ can span $\mathbb{R}^{n}$.}

\begin{proof}
Let $S = \{ \mat{v}_1, \mat{v}_2, \ldots, \mat{v}_n \} \subset \mathbb{R}^n$ be a set of $n$ linearly independent vectors.

The dimension of $\mathbb{R}^n$ is $n$, meaning any basis of $\mathbb{R}^n$ must consist of exactly $n$ vectors.

Since $S$ has $n$ linearly independent vectors, it satisfies the definition of a basis. Therefore, $S$ spans $\mathbb{R}^n$.
\end{proof}

\section{Chapter 3: Vector Spaces}

\subsection{Lab 11: Vector Spaces and Subspaces}

\notes

\begin{enumerate}
\item A vector space is a non-empty set of vectors on which some form of vector addition and multiplication are defined (essentially, a set of vectors defined with a special set of rules for manipulating them). 
\item Dimensional vector space notation is $\mathbb{R}^{n}$, where $n$ is the length of the enclosed vectors.
\item Vector subspaces are nonempty subsets of vector spaces, essentially special sets of vectors for which the following three categories are met:
\begin{enumerate}
\item It contains the zero vector (the additive identity).
\item It's closed under scalar multiplication, meaning that for any vector $\mat{v} \in W$ in subspace $W \in S$ in space $S$, all scalar multiples of $\mat{v}$ are in $W$: $k\mat{v} \in W$.
\item It's closed under vector addition, again meaning that for any two vectors $\mat{v}$ and $\mat{u}$, $\mat{v} + \mat{u} = \mat{u} + \mat{v} \in W$. 
\end{enumerate}
\end{enumerate}

\exercise{1(a)} $\va{u} + \va{v} = (2,2) \in V.$

\exercise{1(b)} Yes, $\va{0}$.

\exercise{1(c)} No. $\va{u} + \va{v} \ne \va{v} + \va{u}$.

\exercise{1(d)} $ku = \begin{pmatrix} 4 & 0 \\ 0 & 16 \end{pmatrix} \in V$.

\exercise{1(e)} $\va{0}$.

\exercise{1(f)} $1\mat{v} = \begin{pmatrix} v_{1} & 0 \\ 0 & v_{4} \end{pmatrix}$

\exercise{1(g)} No. $1\mat{v} \ne \mat{v}$

\exercise{1(h)} $\mathbb{R}^{n}$. Standard multiplication and addition.

\exercise{1(i)} $V = \mathbb{R}^{n}$. Standard addition. For $\va{u} \in V$ and $k \in \mathbb{R}$, let $k\va{u} = \va{0}$.

\exercise{2(a)} $W \in M_{2 \times 2} = \left\{ \left. \begin{pmatrix} x & 0 \\ 0 & y \end{pmatrix} \right| x,y \in \mathbb{R} \right\}$.

\exercise{2(b)}

The general solution set is $S \in \mathbb{R}^{2} = \left\{ \left. \begin{pmatrix} -2x \\ x \end{pmatrix} \right| x \in \mathbb{R} \right\}$. It's closed under addition and multiplication, because for all $\va{a}, \va{b} \in S,$ $\va{a} + \va{b} \in S$ and for all $k \in \mathbb{R}$, $k\va{a} \in S$, including $\va{0}$. Therefore, $S$ is a subspace of $\mathbb{R}^{2}$.

\exercise{2(c)}

Let $V = \left\{\left. f(x) \right| f(x) \ne \text{undefined}, x \in \mathbb{R} \right\}$ and $f(x), g(x) \in S$ be continuous functions defined over $\mathbb{R}$. As any linear combinations of $f(x)$ and $g(x)$ is still continuous, any linear combinations of the elements in $V$ is still in $V$, including $4+5\cos(x)$. Additionally, let 
$$S \in V = \left\{ \left. c_{1} + c_{2} \cos(x) + c_{3} \sin(x) \right| c_1, c_2, c_3 \in \mathbb{R} \right\}$$
be the set of all linear combinations of $1, \cos(x),$ and $\sin(x)$, for which, each element $f(x) \in S$ can be represented by its associated vector of constants 
$$\va{c}_{f(x)} = (c_1, c_2, c_3),$$
as there is a direct mapping between any constant vector $\va{c} \in \mathbb{R}^{3}$ and its corresponding linear combination function $f(x)$. Therefore, as $\mathbb{R}^{3}$ is closed under addition and scalar multiplication, so is $S$.

\theorem{39}{If $V$ is the set of $2 \times 2$ invertible matrices, then $V$ is a vector space under matrix addition and scalar multiplication of matrices.}

\begin{proof}
Untrue. $V$ is not closed under matrix addition. $\mat{I}, -\mat{I} \in V$ but $\mat{I} + (-\mat{I}) = \mat{0} \not\in V$.
\end{proof}

\theorem{40}{The set of $2 \times 2$ symmetric matrices under matrix addition and scalar multiplication of matrices is a vector space.}

\begin{proof}
Let $V \in M_{2 \times 2}$ be the set of symmetric $2 \times 2$ matrices. Let $\mat{A},\mat{B} \in V$. By rules of symmetry, $\mat{A+B}$ is symmetric, so $\mat{A+B} \in S$. Additionally, by rules of symmetry, for scalar $k \in \mathbb{R}$, $k\mat{A} \in S$. Therefore, $V$ is a vector space. 
\end{proof}

\theorem{41}{If $V$ is a vector space with $\va{u}_1$ and $\va{u}_2$ vectors in $V$ then $a_1 \va{u}_1 + a_2 \va{u}_2 + b_1 \va{u}_1 + b_{2} \va{u}_{2} = (a_1 + b_1) \va{u}_1 + (a_2 + b_2) \va{u}_2$ and for any scalars $a_1, a_2, b_1, b_2$.}

\begin{proof}
Where $V$ is a vector space, $\va{u}_{1} + \va{u}_{2} = \va{u}_{2} + \va{u}_{1}$ and $k\va{u} = (ku_{1}, ku_{2}, \dots, ku_n)$ for any scalar $k$. Therefore, scalar multiplication can be written distributively, $(a+b)\va{u} = a\va{u} + b\va{u}$, and vector addition can be written commutatively. Thus, the assertion is true.
\end{proof}

\theorem{42}{If $\mat{A}$ is an $n \times n$ matrix, then the solution set to $Ax = 0$ is a subspace of $\mathbb{R}^{n}$.}

\begin{proof}
Let $S \in \mathbb{R}^{n} = \left\{\left. \mat{x} \right| \mat{Ax} = 0 \right\}.$ If $\mat{A}$ is invertible, $S = \{\va{0}\}$, which is a valid vector space (the only valid single-item vector space). Where $\mat{A}$ is singular, $S$ will be infinitely large. Let $\va{a}, \va{b} \in S$. $k\va{a} \in S$ be solutions. Because a scalar multiplied anywhere in the chain of matrix multiplication produces the same matrix (see below), $\mat{A}(k\mat{a}) = k(\mat{Aa}) = \mat{0} \in S$. 

Furthermore, all solutions $\mat{x} \in S$ are based on relationships between the values of $\mat{x}$, in such a way that $\mat{x}$ can be described internally as a series of base values,
$$\mat{x} = ( x_1, x_2, \dots, x_n ),$$
and all $\mat{v} \in \mat{S}$ must essentially be a scalar multiple of these base values (or in other words, all vectors in $S$ are scalar multiples of each other). Therefore, for any two vectors $\mat{a}$ and $\mat{b}$ in $S$, $\mat{a+b} \in S$. Therefore, $S$ is a subspace of $\mathbb{R}^{n}$. 
\end{proof}

\subtheorem{$k\mat{A} \mat{B} = \mat{A} k \mat{B}$}

\begin{proof}
Scalar multiplication is element-wise and distributed over the entire matrix, so it can occur before or after matrix multiplication. 
\end{proof}

\theorem{43}{If $\mat{A}$ is an $n \times n$ matrix, then the set of linear combinations of the rows of $\mat{A}$ is a subspace of $\mathbb{R}^{n}$.}

\begin{proof}
Let $\va{v}, \va{u} \in \mat{A}$, $S \in \mathbb{R}^{n}$ be the set of linear combinations of rows in $\mat{A}$, and $k \in \mathbb{R}$ be a scalar. $S$ is closed under addition as $\va{v} + \va{u} = \va{u} + \va{v} \in S$. Additionally, $S$ is closed under scalar multiplication, as all $k\va{u} \in S$. Therefore, $S$ is a subspace of $\mathbb{R}^{n}$.
\end{proof}

\theorem{44}{If $S = \{ \va{v}_1, \va{v}_2, \dots, \va{v}_n \}$ is a set of vectors in vector space $V$, then the set of all linear combinations of vectors in $S$ is a subspace of $V$.}

\begin{proof}
Let $W$ be the set of linear combinations of vectors in $S$, $\va{v}, \va{u} \in S$ and $k \in \mathbb{R}$ be a scalar. As $V$ is a vector space, any $k\mat{v}$ or $\va{v} + \va{u}$ are in $W$, and therefore, in $V$. As $W$ is closed under addition and scalar multiplication, $W$ is a subspace of $V$.
\end{proof}

\subsection{Lab 12: Basis Vectors, Null Space, Rank, Column/Rowspace}

\notes

\begin{enumerate}
\item Where a set of vectors $S$ spans a vector space $V$ if all vectors in $V$ can be written as a linear combination of vectors in $S$, $S$ acts as a basis for $V$ (and the vectors of $S$ are basis vectors of $V$).
\begin{enumerate}
\item An example is the cardinal vectors, $(0,1,0), (1,0,0),$ and $(0,0,1)$, which act as basis vectors for $\mathbb{R}^{3}$.
\end{enumerate}
\item The dimension of a vector space (written $\dim V$) is the number of vectors that form its basis set. If the vector space $V$ consists only of $\mat{0}$, then $\dim V = 0$.
\begin{enumerate}
\item If the number of basis vectors is finite, $V$ is called ``finite dimensional.'' If the number if infinite, $V$ is ``infinite dimensional.''
\end{enumerate}
\item A basis for a vector space is not unique, a vector space can have multiple bases. However, all basis sets for the same vector space must contain the same number of vectors (the dimensionality of the basis).
\item If two vector spaces have the same basis, they are in the same vector space.
\item For a given matrix/vector $\mat{A}$, the solution set $S$ to $\mat{Ax=0}$ is known as the nullspace of $\mat{A}$ and the dimension of this null space $\dim S$ is known as the nullility of $\mat{A}$.
\begin{enumerate}
\item The nullspace is considered empty if the only valid solution is the trivial solution, $\mat{x=0}$.
\item The dimension of the nullspace is considered the nullility of the matrix, $\dim (\text{null } \mat{A}) = \text{nullility } \mat{A}$
\end{enumerate}
\item The rowspace/columnspace of a matrix is the set of all linear combinations from the rows/columns of the matrix:
\begin{gather}
\rowsp \mat{A} = \set{ \sum_{i=1}^{n} k_{i} \va{r}_{i\mat{A}} }{ \va{k} \in \mathbb{R}^{n} }. \\
\colsp \mat{A} = \set{ \sum_{i=1}^{n} k_{i} \va{c}_{i\mat{A}} }{ \va{k} \in \mathbb{R}^{n} }.
\end{gather}
\item The dimensionality of the rowspace or columnspace is called the ``rank'' of the matrix: 
\begin{equation}
\rank \mat{A} = \dim( \rowsp \mat{A} ) = \dim( \colsp \mat{A} )
\end{equation}
and it's defined as the maximum number of linearly independent rows or columns in $\mat{A}$.
\begin{enumerate}
\item The rowspace and columnspace of an $n \times n$ invertible matrix must span $\mathbb{R}^{n}$.
\item Basis vectors of the rowspace of $\mat{A}$ are simply the nonzero row vectors of $\rref( \mat{A} )$, and the basis vectors of the columnspace of $\mat{A}$ are the original columns of $\mat{A}$ corresponding to the columns containing leading ones in $\rref( \mat{A} )$.
\end{enumerate}
\item For a matrix $\mat{A} \in \mathbb{R}^{m \times n}$,
\begin{equation}
n = \rank \mat{A} + \nullility \mat{A}
\end{equation}
\end{enumerate}

\exercise{1(a)} $\{ (0,0,1), (0,1,0), (1,0,0), (1,1,1) \}$

\exercise{1(b)} $\{ (0,0,1), (0,1,0) \}$

\exercise{1(c)} $\{ (0,0,2), (0,2,0), (2,0,0) \}$

\exercise{1(d)} $\dim \mathbb{R}^{n} = n$

\exercise{2(a)} $\basis \nullsp \mat{A} = \left\{ \begin{pmatrix} -2 \\ 1 \end{pmatrix} \right\}$

\exercise{2(b)} $\basis \nullsp \mat{B} = \left\{ \begin{pmatrix} -1 \\ -2 \\ 1 \end{pmatrix}  \right\}$

\exercise{2(c)}

\conjecture{Invertible matrices must have an empty nullspace (containing only the trivial solution) and a nullility of 0.}

\exercise{2(d)} $\nullility \mat{A} = \nullility \mat{A}^{T}$ because $\mat{A} = \mat{A}^{T}$, but $\nullility \mat{B} \ne \nullility \mat{B}^{T}$ because $\mat{B} \ne \mat{B}^{T}$.

\exercise{2(e)} $\basis (\nullsp \mat{M}) = \left\{ \begin{pmatrix} 20/11 & 23/11 & -4/11 & 1 \end{pmatrix} \right\}$

\exercise{3(a)} $\basis (\rowsp \mat{A}) = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \rank \mat{A} = 1$

\exercise{3(b)} $\rank \mat{A} + \nullility \mat{A} = 2$

\exercise{3(c)} No. $(0,1) \in \mathbb{R}^{2} \not\in \rowsp \mat{A}$.

\exercise{3(d,e-1)}

\conjecture{Where $\mat{M} \in \mathbb{R}^{n \times n}$ is invertible, $$\vspan( \rowsp \mat{M} ) = \vspan( \colsp \mat{M} ) = \mathbb{R}^{n}.$$}

\exercise{3(e-2)} 

\begin{align*}
\basis(\rowsp \mat{M}) &= \left\{ \begin{pmatrix} 1 \\ 0 \\ 0 \\ -20/11 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \\ -23/11 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \\ 4/11 \end{pmatrix} \right\} \\
\basis(\colsp \mat{M}) &= \left\{ \begin{pmatrix}  1 \\ 0 \\ -2 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 5 \\ 3 \\ 1 \end{pmatrix} \right\}
\end{align*}

\exercise{3(f)} $\rank \mat{M} + \nullility \mat{M} = 4$

\exercise{3(g)} 

\conjecture{The sum of the rank and nullility of a square matrix should be equal to the number of rows and columns, and in general, it should be equal to the number of rows.}

\theorem{45}{$\mat{A}$ is invertible if and only if the nullspace of $\mat{A} = \va{0}$ and $\nullility \mat{A} = 0$.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be invertbile. Therefore, $\mat{Ax=0}$ has only the trivial solution, $\mat{x=0}$. Therefore, $\nullsp \mat{A} = \va{0}$ and $\nullility \mat{A} = 0$. Conversely, if the only solution to $\mat{Ax=0}$ is the trivial solution, the rows and columns of $\mat{A}$ must be linearly independent, making $\mat{A}$ invertbile.
\end{proof}

\theorem{46}{An $n \times n$ matrix $\mat{A}$ is invertible if and only if the rowspace of $\mat{A} = \mathbb{R}^{n}$ and $\rank \mat{A} = n$}

\begin{proof}
To be invertible, the rows and columns of $\mat{A}$ must be linearly independent. As any set of vectors of size $n$ that is linearly independent must span $\mathbb{R}^{n}$, where $\mat{A}$ is an $n \times n$ invertible matrix, $\rowsp \mat{A} = \mathbb{R}^{n}$ and $\rank \mat{A} = n$.
\end{proof}

\theorem{47}{If $\mat{A}$ is an $m \times n$ matrix, then $\rank \mat{A} + \nullility \mat{A} = m$.}

\begin{proof}
Untrue. By the rank-nullility theorem, it would be $n$.
\end{proof}

\theorem{48}{$\rank \mat{A} = \rank \mat{A}^{T}.$}

\begin{proof}
Where $\mat{A} \in \mathbb{R}^{m \times n}$ is a matrix, $$\rank \mat{A} = \dim (\rowsp \mat{A}) = \dim (\colsp \mat{A}).$$
Therefore,
$$\rank \mat{A} = \dim(\colsp \mat{A}^{T}) = \dim(\rowsp \mat{A}^{T})$$
implying that
$$\rank \mat{A} = \rank \mat{A}^{T}.$$
\end{proof}

\subsection{Lab 13: Linear Transformations}

\notes 

\begin{enumerate}
\item A transformation is defined as a function that applies an operation unto a vector/matrix to produce a new vector/matrix, written in the form $T(\va{x}) = \va{y}$ or $T : \va{x} \to \va{y}$ (pronounced ``a transformation such that $\va{x}$ maps to $\va{y}$'').
\item A transformation $T$ is considered linear if it satisfies three qualities for any vectors $u, v \in \dom T$ and scalar $k$:
\begin{enumerate}
\item $T(\va{0}) = \va{0}$
\item $T(\va{v} + \va{u}) = T(\va{v}) + T(\va{u})$
\item $T(k\va{v}) = kT(\va{u})$
\end{enumerate} 
\item The general form of any linear transformation is
\begin{equation}
T(\va{x}) = \mat{A}\va{x},
\end{equation}
where $\mat{A}$ is $T$'s transformation matrix (also known as the standard matrix).
\item A transformation is one-to-one if the transformation consists of distinct and unique mappings (i.e., for every input, there is only one output, and for every output, there is only one input).
\item The kernel of a transformation is the set of vectors that map to $\va{0}$:
\begin{equation}
\ker T = \set{ \va{x} }{ \mat{Ax=0} } = \nullsp \mat{A}, 
\end{equation}
which is equal to the nullspace of its standard matrix.
\item As such, the nullility of a transformation is the dimension of its kernel:
\begin{equation}
\nullility T = \dim(\ker T) = \nullility \mat{A}.
\end{equation}
\item The range of a transformation is the set of possible values that can be produced by the transformation.
\item The rank of a transformation is the dimension of the range of the transformation:
\begin{equation}
\rank T = \dim(\range T)
\end{equation}
\item Common linear transformations are rotation, reflection, and dilation/contraction.
\begin{enumerate}
\item The $2 \times 2$ rotation matrix for a counter-clockwise rotation by $\theta$ is
\begin{equation}
\rot \theta = 
\begin{pmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{pmatrix}
\end{equation}
\item The dilation/contraction by a scalar $k$ (where $k \in (0,1)$ is a contraction and $k \in (1,\infty)$ is a dilation) is $k\mat{I}$.
\end{enumerate}
\end{enumerate}

\theorem{49}{$T_{A}$ is one-to-one if and only if $A$ is invertible.}

\begin{proof}
Let $T(\mat{x}) = \mat{Ax}$ and $\mat{A} \in \mathbb{R}^{n \times n}$. It is given that $\range T = \dom T = \mathbb{R}^{n}$ Suppose $\mat{A}$ is invertible. Therefore, 
\begin{equation*}
\rowsp \mat{A} = \colsp \mat{A} = \range T = \mathbb{R}^{n}.
\end{equation*}
Thus, for input domain $\mathbb{R}^{n}$ to match output domain $\mathbb{R}^{n}$, all values $\mat{x} \in \mathbb{R}^{n}$ can only have a singular mapping to $\mat{y} \in \mathbb{R}^{n}$. 

Furthermore, suppose $\mat{A}$ is singular, and thus, the rows and columns of $\mat{A}$ are linearly dependent. Therefore, there will be infinitely many solutions to $T(\va{x}) = \va{0}$, implying that $T$ cannot be one-to-one. 
\end{proof}

\theorem{50}{If $\mat{A}$ is an invertible matrix, then the kernel of $T_{A}$ is $\mathbb{R}^{n}$.}

\begin{proof}
Untrue. Where $\mat{A}$ is invertible, $\mat{Ax=0}$ only has the trivial solution. Therefore, $\ker T = \{ \va{0} \}$.
\end{proof}

\theorem{51}{If $A$ is an invertible matrix then $\nullility T_{A} = 0$.}

\begin{proof}
Where $\mat{A}$ is invertible, $\mat{Ax=0}$ only has the trivial solution. Therefore, $\nullility T = \nullility \mat{A} = 0$.
\end{proof}

\theorem{52}{If $T_{1} : \mathbb{R}^{n} \to \mathbb{R}^{m}$ and $T_{2} : \mathbb{R}^{m} \to \mathbb{R}^{p}$ are two linear transformations then $T_{2} \circ T_{1}$ is a linear transformation.}

\begin{proof}
True. The composition of $T_{1}$ by $T_{2}$ can be expressed as the composition function
\begin{equation*}
(T_{2} \circ T_{1})(\va{x}) = \mat{A}_{2} \mat{A}_{1} \va{x},
\end{equation*}
which, in itself, is a linear function (as $\mat{A}_{2} \mat{A}_{1}$ comprises another linear transformation matrix representing the combination of both transformations).
\end{proof}

\theorem{53}{$T_{1}(x_{1},x_{2}) = (x_{1} + k_{1}, x_{2} + k_{2})$ is a linear transformation where $k_{1}$ and $k_{2}$ are nonzero scalars. (This transformation represents translation.)}

\begin{proof}
Untrue. Translations break linearity as $T(\va{0}) \ne \va{0}$.
\end{proof}

\subsection{Lab 14: Eigenvalues and Eigenspaces}

\notes

\begin{enumerate}
\item For a square matrix $\mat{M} \in \mathbb{R}^{n \times n}$, the eigenspace of $\mat{M}$ is the vector space of linear combinations of the eigenvectors of $\mat{M}$, and thus, it is equivalent to the nullspace of $\mat{M} - \lambda \mat{I}$ where the eigenvectors of $\mat{M}$ are the basis vectors of this eigenspace.
\begin{equation}
\eigsp \mat{M} = \nullsp ( \mat{M} - \lambda I ).
\end{equation}
\item An $n \times n$ matrix will have $n$ eigenvalues (counting algebraic multiplicity\footnote{Algebraic multiplicity of an eigenvalue is the degree of the eigenvalue as the root of the matrix's characteristic polynomial. For example, where a matrix $\mat{M}$ has an eigenvalue $\lambda$, its characteristic polynomial could have root $0 = (x-\lambda)^{k}$ where $k$ is a scalar, indicating that $\lambda$ has a multiplicity of $k$ and $\lambda$ is $k/n$ of the total $n$ eigenvalues.}).
\item The sum of the eigenvalues of a matrix are equal to the trace of the matrix:
\begin{equation}
\sum \eig \mat{M} = \tr \mat{M}.
\end{equation}
\item The product of the eigenvalues of a matrix are equal to the determinant of the matrix:
\begin{equation}
\prod \eig \mat{M} = \det \mat{M}.
\end{equation}
\item The characteristic polynomial for a matrix will have the roots of the polynomial be the eigenvalues of the matrix, and when interpreted into polynomial-matrix form\footnote{Polynomial-matrix form means that constant terms are multiplied by $\mat{I}_{n}$ and the result is a matrix of $n \times n$.},
\begin{equation}
    p(\mat{M})=\va{0}.
\end{equation}
\item An $n \times n$ matrix will have a characteristic polynomial of degree $n$.
\item The Cayley-Hamilton Theorem can be applied to produce the inverse matrix automatically based on the characteristic polynomial of a matrix:
\begin{equation}
\mat{A}^{-1} = \frac{(-1)^{n-1}}{\det \mat{A}} (\mat{A}^{n-1} + c_{2} \mat{A}^{n-2} + c_{3} \mat{A}^{n-3} + \dots + c_{n} \mat{I}),
\end{equation}
where the form of the characteristic polynomial is
\begin{equation*}
p(x) = c_{1} x^{n} + c_{2} x^{n-1} + \dots + c_{n} x +  c_{n+1}.
\end{equation*}
\end{enumerate}

\theorem{54}{$\mat{A}$ is invertible if and only if all its eigenvalues are nonzero.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be invertible. Therefore, $\det \mat{A} \ne 0$, and likewise, as $\lambda \mat{I} = \va{0}$ where $\lambda = 0$, $\det(\mat{A} - \lambda \mat{I}) \ne 0$, so all eigenvalues of invertible matrices must be nonzero. Conversely, assume $\mat{B} \in \mathbb{R}^{n \times n}$ has only nonzero eigenvalues. This implies that $\det \mat{B} \ne 0$, proving that $\mat{B}$ is invertible. 
\end{proof}

\theorem{55}{If $\lambda$ is an eigenvalue for $\mat{A}$ then $1/\lambda$ is an eigenvalue of $\mat{A}^{-1}$}

\begin{proof}
    Let $\mat{A} \in \mathbb{R}^{n \times n}$ be invertible with eigenvalue $\lambda$ and corresponding eigenvector $\va{x}$. Therefore,
\begin{align*}
  \mat{A} \va{x} &= \lambda \va{x} \\
  \mat{A}^{-1} \mat{A} \va{x} &= \mat{A}^{-1} \lambda \va{x} \\
  \frac{1}{\lambda} \va{x} &= \mat{A}^{-1} \va{x},
\end{align*}
showing that $1/\lambda$ is a corresponding eigenvalue of $\mat{A}^{-1}$ for the same eigenvector $\va{x}$.
\end{proof}

\theorem{56}{If $\lambda$ is an eigenvalue of $\mat{A}$ then $\lambda^{k}$ is an eigenvalue of $\mat{A}^{k}$.}

\begin{proof}
For $k=1$,
$$\mat{A}\va{x} = \lambda \va{x},$$ 
and for $k=-1$, 
$$\va{A}^{-1} \va{x} = \lambda^{-1} \va{x},$$
both are true by assumption. For any $k$, we assume that
$$\mat{A}^{k} \va{x} = \lambda^{k} \va{x},$$ 
and wish to prove that
$$\mat{A}^{k \pm 1} \va{x} = \lambda^{k \pm 1} \va{x}.$$
By the induction hypothesis, we obtain that
$$\mat{A}^{\pm 1}(\lambda^{k} \va{x}) = \lambda^{k} \mat{A}^{\pm 1} \va{x},$$
which can then become
$$= \lambda^{k \pm 1}(\lambda \va{x}) = \lambda^{k \pm 1}$$ 
\end{proof}

\theorem{57}{If $\lambda$ is an eigenvalue for $\mat{A}$ then $\lambda$ is an eigenvalue of $\mat{A}^{T}$.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ with eigenvalue $\lambda$. As $\det \mat{A} = \det \mat{A}^{T}$ and $\lambda \mat{I}$ is diagonal,
$$\det(\mat{A} - \lambda \mat{I}) = \det(\mat{A}^{T} - \lambda \mat{I}),$$
therefore $\lambda$ is an eigenvalue of $\mat{A}^{T}$.
\end{proof}

\theorem{58}{The eigenvalues of a triangular matrix (upper or lower) are the entries on the main diagonal.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be triangular. Therefore, as $\lambda \mat{I}$ is diagonal, $\mat{A} - \lambda \mat{I}$ is also triangular. Therefore, $\det(\mat{A} - \lambda \mat{I})$ is the product of the elements on the main diagonal, so the eigenvalues of $\mat{A}$ will be values that can cause any value on the main diagonal of $\mat{A}$ to become zero from subtraction, which would jut be the values on the main diagonal.   
\end{proof}

\theorem{59}{If $\mat{A}$ is an $n \times n$ matrix with $n$ distinct eigenvalues then all the corresponding eigenvectors are linearly independent.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$. Suppose $\mat{A}$ has distinct eigenvalues. Therefore, for every $\lambda \in \eig \mat{A}$, the eigenspace for $\lambda$ must not intersect with the eigenspaces for any other eigenvalue, and by that assertion, the associated eigenvectors must not be linearly dependent.
\end{proof}

\subsection{Lab 15: Markov Chains}

\notes

\begin{enumerate}
\item Markov models are statistical models used to describe a series of states and the transitions (usually random transitions) between those states (like a graph).
\item Much like a graph, markov models can be represented using a matrix, where $M_{ij}$ is the probability of transition from $i \to j$.
\item The dormant eigenvalue of a markov matrix is the largest eigenvalue of the matrix, which is the largest eigenvalue of the matrix.
\item The long-run solution state of a markov chain is stable (convergent to a singular state) if the markov matrix has a dormant eigenvalue of exactly 1. 
\end{enumerate}

\section{Chapter 4: Orthogonality}

\subsection{Lab 16: Inner Product Spaces}

\notes

\begin{enumerate}
  \item A euclidean inner product $<\va{u}, \va{v}>$ is equivalent to a dot product $\va{u} \vdot \va{v}$. A vector space with a defined inner product is called an inner product space.
\begin{equation}
<\va{u},\va{v}> = \va{u} \vdot \va{v} = \sum_{i=1}^{n} u_{i} v_{i}
\end{equation}
\item The normalization (norm) or magnitude $||\va{v}||$ of a vector $\va{v}$ is the square root of a vector's euclidean inner product with itself
\begin{equation}
||\va{v}|| = \sqrt{<\va{v},\va{v}>}
\end{equation}
\item Unit vectors have a magnitude of 1, and are denoted with a hat: $\vu{v}$.
\item Two vectors are considered orthogonal if their inner/dot product is 0.
\begin{enumerate}
\item In $\mathbb{R}^{2}$, orthogonality is analogous with perpendicularity.
\end{enumerate}
\item Two vectors are considered orthonormal if they are orthogonal and they are both unit vectors.
\item A set is considered orthogonal if all of its enclosed vectors are orthogonal to each other and orthonormal if all of its enclosed vectors are unit vectors.
\item For a given subset $V$ of a larger set $S$, its orthonormal complement $W$ is the subset containing all vectors $\va{v} \in S$ in the larger set that are orthogonal to all vectors in the other subset $V$.
\item The Gram-Schmidt Process is an algorithm for finding an orthonormal set of vectors that spans an inner product space given a starting subset, following the recursive formula
\begin{equation}
\va{u}_{n} = \va{v}_{n} - \sum_{i=1}^{n-1} \proj_{\va{u}_{i}} \va{v}_{n}, \hspace{1em} \va{e}_{n} = \frac{\va{u}_{n}}{||\va{u}_{n}||},
\end{equation}
for input vectors $\{\va{v}_{1}, \va{v}_{2}, \dots, \va{v}_{k} \}$ producing the required set of ortogonal vectors $\{ \va{u}_{1}, \va{u}_{2}, \dots \va{u}_{k} \}$ and the normalized vectors $\{\va{e}_{1}, \va{e}_{2}, \dots, \va{e}_{k} \}$.
\end{enumerate}

\theorem{60}{If $\va{u}$ and $\va{v}$ are orthogonal vectors in $V$, then $$||\va{u} + \va{v}||^2 = ||\va{u}||^2 + ||\va{v}||^2.$$}

\begin{proof}
By definition,
$$||\va{u}||^2 + ||\va{v}||^2 = \sum_{i=1}^{n} u_{i}^2 + \sum_{i=1}^{n} v_{i}^2 = \sum_{i=1}^{n} (u_i^2 + v_i^2),$$
and similarly,
$$||\va{u} + \va{v}||^2 = \sum_{i=1}^{n} (\va{u} + \va{v})_{i}^2.$$
Therefore, we must prove
$$\sum_{i=1}^{n} (u_{i}^2 + v_i^2) = \sum_{i=1}^{n} (u_i + v_i)^{2},$$
which, expanded, becomes proving that
$$\sum u_i v_i = 0,$$
and this is true by definition as $\va{u}$ and $\va{v}$ are orthogonal.
\end{proof}

\theorem{61}{If $\va{u}$ and $\va{v}$ are vectors in $V$ then $$|<\va{u}, \va{v}>| < ||\va{u}||||\va{v}||.$$}

\begin{proof}
This relation can be rewritten into its definitions as
$$\left|\sum_{i=1}^n u_i v_i\right| < \sqrt{\sum_{i=1}^n (u_i v_i)^2 },$$
or, even further, can be expanded based on the definition of absolute value into
$$\textstyle \left(\sqrt{\sum_{i=1}^n u_i v_i}\right)^2 < \sqrt{\sum_{i=1}^n (u_i v_i)^2 },$$
and thus, this relation is untrue. The two are equivalent.
\end{proof}

\theorem{62}{If $\va{u}$ and $\va{v}$ are vectors in $V$ then $$||\va{u} + \va{v}|| \le ||\va{u}|| + ||\va{v}||.$$}

\theorem{63}{If $\va{u}$ and $\va{v}$ are vectors in $V$ then $$||\va{u} + \va{v}||^2 + ||\va{u} - \va{v}||^2 = 2(||\va{u}||^2 + ||\va{v}||^2).$$}

\theorem{64}{If $\{ \va{v}_{1}, \va{v}_{2}, \dots, \va{v}_{n} \}$ is an orthonormal set of vectors in $V$, then $$||k_{1}\va{v}_{1} + k_{2}\va{v}_{2} + \dots + k_{n} \va{v}_{n} ||^2 = |k_{1}|^2 + |k_{2}|^2 + \dots + |k_{n}|^2.$$}

\theorem{65}{Every orthonormal set of vectors is linearly independent.}

\theorem{66}{If $T:\mathbb{R}^{n} \to \mathbb{R}^{n}$ is the linear transformation defined by the left multiplication of the $n \times n$ matrix $\mat{A}$ and vector $\va{v}$ is in the range of $T$ then $\va{v}$ is orthogonal to the nullspace of $\mat{A}$.}

\end{document}
