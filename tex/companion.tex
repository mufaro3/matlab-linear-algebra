\documentclass[12pt]{article}
\usepackage{parskip, multicol}
\usepackage{amssymb, amsmath, amsthm, physics}
\usepackage{hyperref}

\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\exercise}[1]{\textbf{EXERCISE #1}\label{#1}}
\newcommand{\theorem}[2]{\textbf{THEOREM #1.} #2}
\newcommand{\conjecture}[1]{\textbf{CONJECTURE:} #1}
\newcommand{\notes}{\textbf{NOTES}}
\newcommand{\rref}{\text{RREF}}
\newcommand{\eig}{\text{eig }}
\newcommand{\ct}[1]{\mat{\overline{#1}}^{T}}
\newcommand{\nullsp}{\text{null }}
\newcommand{\nullility}{\text{nulllility }}
\newcommand{\rowsp}{\text{rowsp }}
\newcommand{\colsp}{\text{colsp }}
\newcommand{\subtheorem}[1]{\textbf{SUBTHEOREM:} #1}
\newcommand{\basis}{\text{basis }}
\newcommand{\set}[2]{\left\{\left.#1\right|#2\right\}}

\begin{document}

\textbf{\Large COMPANION NOTES AND SOLUTIONS}

by Mufaro Machaya

\tableofcontents
\newpage

\section{Chapter 1: Matrices}

\subsection{Labs 0 and 1: Introduction and Matrix Basics}

\notes

\begin{enumerate}
\item Matrix addition is commutative, so addition in any order produces the same result.
\begin{equation}
  \mat{A} + \mat{B} = \mat{B} + \mat{A}
\end{equation}
\item Scalar multiplication is applied on all elements of the given matrix as a regular multiplication. For example, for $3 \times 3$ matrix $\mat{A}$ and scalar $s$:
\begin{equation}
    s\mat{A} =
\begin{pmatrix} 
  sA_{11} & sA_{12} & sA_{13} \\
  sA_{21} & sA_{22} & sA_{23} \\
  sA_{31} & sA_{32} & sA_{33}
\end{pmatrix}.
\end{equation}
\item A transposition is essentially swapping your rows and columns (do not, however, mistake it for a rotation), so $2 \times 3$ matrix $\mat{A}$ would produce $3 \times 2$ matrix $\mat{A}^{-1}$:
\begin{equation}
  \begin{pmatrix}
    A_{11} & A_{12} & A_{13} \\
    A_{21} & A_{22} & A_{23}
  \end{pmatrix}^{T} =
  \begin{pmatrix}
    A_{11} & A_{21} \\
    A_{12} & A_{22} \\
    A_{13} & A_{23}
  \end{pmatrix}
\end{equation}
\item Matrix multiplication essentially computes the dot products of the rows of the first matrix with the columns of the second matrix, thus why the number of rows of the first matrix must equal the number of columns of the second. For example, for $2 \times 3$ matrix $\mat{A}$ and $3 \times 2$ matrix $\mat{B}$, the resulting matrix would be
\begin{equation*}
  \mat{A} \mat{B} =
\begin{pmatrix}
  \mat{A}_{r=1} \cdot \mat{B}_{c=1} & \mat{A}_{r=1} \cdot \mat{B}_{c=2} \\
  \mat{A}_{r=2} \cdot \mat{B}_{c=1} & \mat{A}_{r=2} \cdot \mat{B}_{c=2}
\end{pmatrix}
  \footnote{$M_{r=m}$ denotes ``the vector comprised of the elements of row $m$ in matrix $\mat{M}$'' and $M_{c=n}$ denotes ``the vector comprised of the elements of column $n$ in matrix $\mat{M}$.''}
\end{equation*}
\begin{equation}
    =
\begin{pmatrix}
  A_{11} B_{11} + A_{12} B_{21} + A_{13} B_{31} &
  A_{11} B_{12} + A_{12} B_{22} + A_{13} B_{32} \\
  A_{21} B_{11} + A_{22} B_{21} + A_{23} B_{31} &
  A_{21} B_{12} + A_{22} B_{22} + A_{23} B_{32}
\end{pmatrix}
\end{equation}
\item Matrix multiplication is not commutative, so multiplying matrices in different orders will produce different results.
\begin{equation}
  \mat{A} \mat{B} \neq \mat{B} \mat{A}
\end{equation}
\item Matrix addition and scalar multiplication are commutative with transposition, but multiplication is not.
\begin{align}
  (\mat{A} + \mat{B})^T &= \mat{A}^T + \mat{B}^T \\
  (s\mat{A})^T &= s\mat{A}^T \\
  (\mat{A}\mat{B})^T &\neq \mat{A}^T \mat{B}^T
\end{align}
\item The trace of a matrix is the sum of the elements on its main diagonal.
\item The trace is commutative with matrix addition (i.e., $\tr(\mat{A} + \mat{B}) = \tr(\mat{A}) + \tr(\mat{B})$), but non-commutative with matrix multiplication. 
\item Matrix multiplication within trace in either order will produce the same result,
\begin{equation}
\tr(\mat{AB}) = \tr(\mat{BA})
\end{equation}
\end{enumerate}

\subsection{Lab 2: A Matrix Representation of Linear Systems}

\exercise{2(g)}

Given
\begin{equation*}
  \mat{M} = \begin{pmatrix}
    1 & 2 & 0 \\
    0 & 0 & 3 \\
    0 & 1 & 0
  \end{pmatrix},
\end{equation*}
we can produce $\mat{M} \to \mat{I}_3$ via:
\begin{enumerate}
\item Divide row $3$ by 3.
\item Add multiples of -2 from row 2 to row 1.
\item Swap rows 2 and 3\footnote{This was originally wrong and placed first. You need to swap last because matrix multiplication is right-associative, so the rightmost operation occurs first, not last. In other words, a ``correct'' way to approach this would be to do the entire process in reverse when multiplying.}.
\end{enumerate}
This effect can be produced by the following elementary matrices. 

The swap matrix can be created from emulation with $I_3$:
\begin{equation*}
\mat{E}_{swap} = 
\begin{pmatrix}
 1 & 0 & 0 \\
 0 & 0 & 1 \\
 0 & 1 & 0
\end{pmatrix},
\end{equation*}
similarly, the division matrix would be
\begin{equation*}
\mat{E}_{div} =
\begin{pmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & \frac{1}{3}
\end{pmatrix},
\end{equation*}
and lastly, the additive matrix would be
\begin{equation*}
\mat{E}_{add} =
\begin{pmatrix}
1 & -2 & 0 \\
0 & 1  & 0 \\
0 & 0  & 1
\end{pmatrix}.
\end{equation*}
Altogether, the following multiplication should hold true:
\begin{equation*}
\mat{E}_{div} \mat{E}_{add} \mat{E}_{swap} \mat{M} = \mat{I}_{3}
\end{equation*}
\newpage

\exercise{2(h)}

Given
\begin{align*}
x + 2y &= 4 \\
3z &= 6 \\
y &= 8,
\end{align*}
we can produce the following coefficient matrix $\mat{A}$ and constant vector $\mat{B}$:
\begin{equation*}
\mat{A} = 
\begin{pmatrix} 
1 & 2 & 0 \\ 
0 & 0 & 3 \\ 
0 & 1 & 0 
\end{pmatrix} 
\hspace{0.10\linewidth} 
\mat{B} = 
\begin{pmatrix} 
4 \\ 6 \\ 8
\end{pmatrix},
\end{equation*}
and given that $\mat{A} = \mat{M},$ from exercise 2(g) the solutions can be calculated by performing the RREF calculations on $\mat{B}$ to produce solution vector $\mat{S}$:
\begin{equation*}
\mat{E}_{div} \mat{E}_{add} \mat{E}_{swap} \mat{B} = \mat{S},
\end{equation*}
yielding
\begin{equation*}
\mat{S} = \begin{pmatrix} -12 \\ 8 \\ 2 \end{pmatrix},
\end{equation*}
showing that $x=-12, y = 8, z=2$.

\exercise{2(i)}

This isn't solvable because the best RREF form is
\begin{equation*}
\mat{[M | B]} = 
\begin{pmatrix}
1 & 0 & -6 & 4 \\
0 & 1 &  3 & 6 \\
0 & 0 &  0 & 6 
\end{pmatrix}
\end{equation*}
or, in other words, equations (2) and (3) contradict each other.

\exercise{2(j)}

The initial state of the system is
\begin{equation*}
\begin{pmatrix}
1 & 2 & 0 & 4 \\
0 & 1 & 3 & 6 \\
0 & 2 & 6 & 12
\end{pmatrix},
\end{equation*}
and the RREF of this system becomes
\begin{equation*}
\begin{pmatrix}
1 & 0 & -6 & -8 \\
0 & 1 & 3 & 6 \\
0 & 0 & 0 & 0
\end{pmatrix}.
\end{equation*}
Again, this system is not solvable as equations 2 and 3 are equivalent, resulting in this system not containing enough information to be adequately solved, but the reduced echelon form of the coefficient matrix is the same as in exercise 2(i).

\subsection{Lab 3: Powers, Inverses, and Special Matrices}

\notes

\begin{enumerate}
\item A matrix multiplied by its transposition will produce a gram matrix which is symmetric (does not change on transposition).
\begin{equation}
  \mat{M}^T \mat{M} = (\mat{M}^T \mat{M})^T \neq \mat{M} \mat{M}^T = (\mat{M} \mat{M}^T)^T
\end{equation}
\item A matrix multiplied by its inverse, in any order, will produce an identity matrix.
\begin{equation}
  \mat{M} \mat{M}^{-1} = \mat{M}^{-1} \mat{M} = \mat{I}_{n}
\end{equation}
\item A matrix inverted twice produces itself.
\begin{equation}
(\mat{M}^{-1})^{-1} = \mat{M}
\end{equation}
\item Matrix multiplication is not commutative with inversion, however, they are commutative in alternating order.
\begin{equation}
(\mat{AB})^{-1} = \mat{B}^{-1} \mat{A}^{-1} \neq (\mat{BA})^{-1} = \mat{A}^{-1} \mat{B}^{-1}
\end{equation}
\item Matrix inversion is commutative with transposition.
\begin{equation}
(\mat{M}^T)^{-1} = (\mat{M}^{-1})^{T}
\end{equation}
\item A square matrix $\mat{M}$ is
\begin{enumerate}
\item Symmetric when $\mat{M} = \mat{M}^T$
\item Diagonal when $\mat{M}_{ij} = 0$ if $i \neq j$
\item Upper triangular when $\mat{M}_{ij} = 0$ if $i > j$ or lower triangular when $i < j$.
\end{enumerate}
\item Assuming a matrix to be square, a matrix plus its transposition is symmetric.
\begin{equation}
\mat{M} + \mat{M}^T = (\mat{M} + \mat{M}^T)^T
\end{equation}
\end{enumerate}

\theorem{1}{The inverse of an elementary matrix is an elementary matrix.}

\begin{proof}
There are three elementary operations to be considered: row swaps, multiplication/division by a scalar, and addition/subtraction of a row (by a scalar). 

Let $\mat{E}_{swap}$ be the elementary matrix produced by a swap. By gaussian elimination, $\mat{E}_{swap}$ is its own inverse,
$$\mat{E}_{swap}^{-1} = \mat{E}_{swap},$$
as it can be undone (returned to the identity matrix $\mat{I}$) by repeating the swap that produced it. 

Let $\mat{E}_{scale}$ be the elementary matrix produced by the multiplication of any scalar $s$. This multiplication by $s$ can be undone by division by $s$, so $\mat{E}^{-1}_{scale}$ would be the elementary matrix produced by the divison of $s$. 

Let $\mat{E}_{add}$ be the elementary matrix produced by the addition by any scalar multiple $s$ from one row to another. As addition can be undone by subtraction, the addition by a scalar multiple $s$ can be undone by the subtraction of the same values.
\end{proof}

\theorem{2}{If $\mat{A}$ is invertible then the reduced row echelon form of $\mat{A}$ is $\mat{I}$.}

\begin{proof}
If $\mat{A} \in \mathbb{R}^{n \times n}$ is invertible, then there exists some composite matrix $\mat{E}$ as the product of elementary operations that satisfies the following two equations:
\begin{equation*}
\mat{EA} = \mat{I}, \hspace{0.25\textwidth} \mat{EI} = \mat{A}^{-1}.
\end{equation*}
These two equations correspond to the Gauss-Jordan elimination process, showing the transformation results on the left and right sides, and the left equation will reduce $\mat{A}$ into its reduced row echelon form. Therefore, the RREF form of $\mat{A}$ must be $\mat{I}$.
\end{proof}

\theorem{3}{If the reduced row echelon form of $\mat{A}$ is $\mat{I}$, then $\mat{A}$ is invertible.}

\begin{proof}
As shown in theorem 2, for $\mat{A} \in \mathbb{R}^{n \times n}$ to satisfy $\rref(\mat{A}) = \mat{I}$, there must be a composite matrix $\mat{E} \in \mathbb{R}^{n \times n}$ such that $\mat{EA} = \mat{I}$, and therefore, this same matrix will simultaneously produce $\mat{EI} = \mat{A}^{-1}$ by Gauss-Jordan elimination, so $\mat{A}$ must be invertible.
\end{proof}

\theorem{4}{$\mat{A}$ is a square invertible matrix if and only if $\mat{A}$ can be written as the product of elementary matrices.}

\begin{proof}
Again, for $\mat{A} \in \mathbb{R}^{n \times n}$ to be invertible, it must satisfy $\mat{EA} = \mat{I}$ and $\mat{EI} = \mat{A}^{-1}$ for some product matrix $\mat{E}$ of elementary matrices in form
$$\mat{E} = \mat{E}_{k} \mat{E}_{k-1} \dots \mat{E}_{2} \mat{E}_{1} = \left( \prod_{n=1}^{k} \mat{E}_{n} \right)^{\sim}.\footnote{The tilde denotes that this product is in reverse order.}\footnote{From here on out, further matricies will be described as "composite."}$$
Therefore, $\mat{A} = \mat{E}^{-1}$, and as the inverse of elementary matrices is elementary, $A$ inherently must be the product of elementary matrices if it is invertible. Furthermore, all composite matrices are inherently invertible because all elementary matrices are invertible, so altogehter, $\mat{A}$ is square and invertible if and only if its a composite product of elementary matrices. 
\end{proof}

\theorem{5}{If $\mat{A}$ is invertible then $\mat{A}^{k}$ is invertible for any natural number $k$.}

\begin{proof}
If $\mat{A} \in \mathbb{R}^{n \times n}$ is invertible, then $\mat{A} \mat{A}^{-1} = \mat{A}^{-1} \mat{A} = I$, so the multiplication between them is commutative, meaning that 
$$\mat{A}^{k} (\mat{A}^{-1})^{k} = (\mat{A} \mat{A}^{-1})^{k} = \mat{I}^{k} = \mat{I}.$$
\end{proof}

\theorem{6}{If $\mat{A}$ is symmetric so is $\mat{A}^{T}$.}

\begin{proof}
By the definition of symmetry, $\mat{A} = \mat{A}^{T}$, so $\mat{A}^{T}$ is symmetric.
\end{proof}

\theorem{7}{If $\mat{A}$ is a symmetric invertible matrix then $\mat{A}^{-1}$ is symmetric.}

\begin{proof}
Transposition is commutative with inversion, so $(\mat{A}^{T})^{-1} = (\mat{A}^{-1})^{T}.$ Therefore, if $\mat{A} = \mat{A}^{T}$ then $\mat{A}^{-1} = (\mat{A}^{T})^{-1} = (\mat{A}^{-1})^{T}$.
\end{proof}

\theorem{8}{If $\mat{A}$ and $\mat{B}$ are symmetric matrices of the same size then $\mat{A} + \mat{B}$ is symmetric.}

\begin{proof}
$\mat{A} = \mat{A}^{T}$ and $\mat{B} = \mat{B}^{T}$, so 
$$\mat{A} + \mat{B} = \mat{A}^{T} + \mat{B}^{T} = \mat{A}^{T} + \mat{B} = \mat{A} + \mat{B}^{T}.$$
Matrix multiplication is inherently commutative with transposition, so $(\mat{A} + \mat{B})^{T} = \mat{A}^{T} + \mat{B}^{T} = \mat{A} + \mat{B}$.
\end{proof}

\theorem{9}{If $\mat{A}$ and $\mat{B}$ are symmetric matrices of the same size, then $\mat{AB}$ is symmetric.}

\begin{proof}
As $\mat{A} = \mat{A}^{T}$ and $\mat{B} = \mat{B}^{T}$, then $\mat{AB} = \mat{A}^{T} \mat{B}^{T} = \mat{A}^{T} \mat{B} = \mat{A} \mat{B}^{T}$, and as matrix multiplication is commutative with transposition in a reverse order (i.e., $(\mat{AB})^{T} = \mat{B}^{T} \mat{A}^{T}$ and vice versa), because both $\mat{A}$ and $\mat{B}$ are symmetric, the matrix multiplication and transposition in this case are fully commutative, so $\mat{AB} = \mat{BA} = \mat{A}^{T} \mat{B}^{T} = (\mat{AB})^{T} = \mat{B}^{T} \mat{A}^{T} = (\mat{BA})^{T}$.
\end{proof}

\theorem{10}{If $\mat{A}$ is a square matrix then $\mat{A} + \mat{A}^{T}$ is symmetric.}

\begin{proof}
Transposition is commutative with addition, thus making it distributive: $(\mat{A} + \mat{A}^{T})^{T} = \mat{A}^{T} + (\mat{A}^{T})^{T} = \mat{A}^{T} + \mat{A}.$
\end{proof}

\theorem{11}{The sum of upper triangular matrices is upper triangular.}

\begin{proof}
This is intuitively true based on how matrix addition works. Let $\mat{A} \in \mathbb{R}^{n \times n}$ and $\mat{B} \in \mathbb{R}^{n \times n}$. All $A_{ij} + B_{ij} = 0$ where $i > j$ as $A_{ij} = 0$ and $B_{ij} = 0$. 
\end{proof}

\subsection{Lab 4: Graph Theory and Adjacency Matrices}

\notes

\begin{enumerate}
\item Graphs are comprised of nodes (points/places/etc) and edges (the relationships between the nodes, like paths/connections).
\item An edge and the node connected to it are called an indicent. Two nodes connected by an edge are considered adjacent.
\item Graphs can be represented as matrices in the form of an adjacency matrix, which are symmetric, square matrices such that indexing the matrix for the points in either order will produce the number of connections between the nodes at that depth, such as:
\begin{equation*}
\mat{M} = \begin{pmatrix}
0 & 1 & 1 & 0 \\
1 & 0 & 0 & 1 \\
1 & 0 & 0 & 1 \\
0 & 1 & 1 & 0
\end{pmatrix}.
\end{equation*}
For example, let $\mat{M}$ be a the adjacency matrix storing two points, $a$ and $b$ (where $a$ and $b$ each correspond to indices of the matrix respectively). The number of connections $n$ long (passing through $n$ edges) will be found at
\begin{equation}
\mat{M}^{n}(a,b) \hspace{0.125\textwidth} \text{and} \hspace{0.125\textwidth} \mat{M}^{n}(b,a).
\end{equation}  
\end{enumerate}

\subsection{Lab 5: Permutations and Determinants}

\notes

\begin{enumerate}
\item Given the number of options/variables, $n$, the number of permutations given $n$ will be $n!$.
\item The determinant of a $2 \times 2$ matrix is
\begin{equation}
\det \begin{pmatrix}
  a_{11} & a_{12} \\
  a_{21} & a_{22}
\end{pmatrix} = a_{11} a_{22} - a_{12} a_{21}.
\end{equation}
\item If $\det \mat{M} = 0$ then $\mat{M}$ is not invertible.
\item For scalar $k$ and $n \times n$ matrix $\mat{M}$, $\det ( k \mat{M} ) = k^{n} \det \mat{M}$
\item The determinant of the identity matrix, of any size, is always $1$.
\begin{equation}
\det \mat{I}_{n} = 1 \text{ for } n \in \{ 1, 2, \dots  \}
\end{equation}
\item The determinant of a matrix is equal to the determinant of its transposition. 
\begin{equation}
\det \mat{M} = \det \mat{M}^{T}
\end{equation}
\item Matrix multiplication is fully commutative with determinants, so
\begin{equation}
\det ( \mat{AB} ) = \det ( \mat{BA} ) = \det \mat{A} \det \mat {B}
\end{equation}
\item Again, matrix addition is commutative, so 
\begin{equation}
\det ( \mat{A} + \mat{B} ) = \det ( \mat{B} + \mat{A} ),
\end{equation}
but matrix addition is not commutative with calculating determinants, so
\begin{equation}
\det ( \mat{A} + \mat{B} ) \neq \det \mat{A} \det \mat{B}.
\end{equation}
\item The determinant of any triangular matrix (upper/lower triangular and diagonal matrices) are just the product of the main diagonal.
\item An alternative way to calculate the determinant is based on the trace of a matrix using the Cayley-Hamilton theorem\footnote{\href{https://en.wikipedia.org/wiki/Cayley\%E2\%80\%93Hamilton_theorem}{Cayley–Hamilton theorem on Wikipedia.}} for a $2 \times 2$ matrix $\mat{M}_{2}$,
\begin{equation}
\det \mat{M}_{2} = \frac{ (\tr \mat{M}_{2})^{2} - \tr \mat{M}_{2}^{2} }{2},
\end{equation}
and for a $3 \times 3$ matrix $\mat{M}_{3}$,
\begin{equation}
\det \mat{M}_{3} = \frac{ (\tr \mat{M}_{3})^3 - 3\tr \mat{M}_{3}^2 \tr \mat{M}_{3} + 2 \tr \mat{M}^3 }{6}
\end{equation} 
\end{enumerate}

\exercise{2(a-c)}

Let $\mat{E}$ be an elementary matrix produced by one of the following operations. 
\begin{enumerate}
\item If $\mat{E}$ is produced by scalar $k$ in a row-scaling operation, $\det \mat{E} = k.$
\item If $\mat{E}$ is produced by a scalar addition, $\det \mat{E} = 1$ (unchanged).
\item If $\mat{E}$ is produced by a row swap, $\det \mat{E} = -1$.
\end{enumerate}

\theorem{12}{If $\det \mat{A} \neq 0$ then $\mat{A}$ is invertible.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$. If $\det \mat{A} \neq 0$ then $\mat{A}$ can be reduced to $\mat{I}_{n}$ using only elementary operations, implying $\mat{A}$ satisfies $\mat{EA} = \mat{I}$ for some composite/product matrix of elementary matrices, $\mat{E}$. By definition, $\mat{E} = \mat{A}^{-1}$, making $\mat{A}$ invertible.
\end{proof}

\theorem{13}{If $\mat{A}$ is invertible then $\det \mat{A} \neq 0$.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be an invertible matrix. $\mat{A}$ therefore satisfies $\mat{EA} = \mat{I}$ for some matrix $\mat{E}$ comprised of the product of elementary matrices. By definition, $\mat{A} = \mat{E}^{-1}$, and as the inverse of the product of elementary matrices will be the product of elementary matrices, $\mat{A}$ is also a product of elementary matrices. As the determinant of any elementary matrix is nonzero, and the determinant of a product is the product of determinants, $\det \mat{A} = \det \mat{E}^{-1} \neq 0$. 
\end{proof}

\theorem{14}{If $\mat{A}$ and $\mat{B}$ are invertible matrices of the same size, then $\mat{A} + \mat{B}$ is invertible.}

\begin{proof}
This is incorrect, because $\det \mat{A} + \det \mat{B} \neq \det ( \mat{A} + \mat{B} )$. For example, if $\mat{A} = -\mat{B}$. Let $\mat{A} = -\mat{I}_{2}$ and $\mat{B} = \mat{I}_{2}$. $\det \mat{A} = \det \mat{B} = 1$, but $\det ( \mat{A} + \mat{B} ) = 0$.
\end{proof}

\theorem{15}{If $\mat{A}$ is a square matrix, then $\det \mat{A} = \det \mat{A}^{T}.$}

\begin{proof}
Let $\mat{M} \in \mathbb{R}^{n \times n}. \det \mat{M}$ is defined via permutations of row and column indices of $\mat{M}$, so swapping the rows and columns will not affect $\det \mat{M}$.
\end{proof}

\theorem{16}{If $\mat{A}$ and $\mat{B}$ are matrices of the same size then $\mat{A}$ and $\mat{B}$ are invertible if and only if $\mat{AB}$ is invertible.}

\begin{proof}
Let $\mat{A}, \mat{B} \in \mathbb{R}^{n \times n}$. If $\mat{AB}$ is invertible, then $\det \mat{AB} \neq 0$, and as $\det \mat{AB} = \det \mat{A} \det \mat{B}$, $\det \mat{A} \neq 0$ and $\det \mat{B} \neq 0$. Therefore, $\mat{A}$ and $\mat{B}$ are invertible. Opposingly, if $\mat{A}$ and $\mat{B}$ are invertible, $\det \mat{A} \ne 0$ and $\det \mat{B} \ne 0$. Therefore, $\det \mat{AB} \ne 0$, so $\mat{AB}$ must be invertible.
\end{proof}

\subsection{Lab 6: 4 $\times$ 4 Determinants and Beyond}

\notes

\begin{enumerate}
\item The equation for calculating matrices higher than $2 \times 2$ follows a row or column-based expansion in form
\begin{equation}
\det \mat{M} \bigg\rvert_{i=c} = \sum^{n}_{j = 1} (-1)^{c+j} a_{cj} \det( \mat{M}^{*}_{cj} \in \mat{M} )
\end{equation}
for a row-based expansion at row $c$ or
\begin{equation}
\det \mat{M} \bigg\rvert_{j=c} = \sum^{n}_{i = 1} (-1)^{i+c} a_{ic} \det( \mat{M}^{*}_{ic} \in \mat{M} )
\end{equation}
for a column-based expansion, given matrix $\mat{M} \in \mathbb{R}^{n \times n}$, and $\mat{M}^{*}_{ij}$ denotes a submatrix anchored by $(i,j)$, meaning that row $i$ and column $j$ are removed from $\mat{M}$ to produce $\mat{M}^{*}_{ij}$. 

For example, for a $3 \times 3$ matrix $\mat{A}$,
\begin{align*}
\det \mat{A} \bigg\rvert_{i=1} 
&= \sum_{j = 1}^{3} (-1)^{1+j} a_{1j} \det ( \mat{A}^{*}_{1j} \in \mat{A} ) \\
&= a_{11} \det \begin{pmatrix}
  a_{22} & a_{23} \\
  a_{32} & a_{33}
\end{pmatrix} -
a_{12} \det \begin{pmatrix}
  a_{21} & a_{23} \\
  a_{31} & a_{33}
\end{pmatrix} +
a_{13} \det \begin{pmatrix}
  a_{21} & a_{22} \\
  a_{31} & a_{32}
\end{pmatrix}
\end{align*}
\end{enumerate}

\newpage
\exercise{1(a)}

\begin{align*}
M_{41} &= \det \begin{pmatrix}
  1 & 0 & 0 \\
  2 & 1 & 0 \\
  1 & 3 & 1
\end{pmatrix}
= \det \begin{pmatrix}
  1 & 0 \\
  3 & 1
\end{pmatrix}
 = 1 \\
M_{42} &= \det \begin{pmatrix}
  1 & 0 & 0 \\
  1 & 1 & 0 \\
  2 & 3 & 1
\end{pmatrix}
 = \det \begin{pmatrix}
   1 & 0 \\
   3 & 1
\end{pmatrix}
= 1 \\
M_{43} &= \det \begin{pmatrix}
  1 & 1 & 0 \\
  1 & 2 & 0 \\
  2 & 1 & 1
\end{pmatrix}
= \det \begin{pmatrix}
  2 & 0 \\
  1 & 1
\end{pmatrix} -
\det \begin{pmatrix}
  1 & 0 \\
  2 & 1
\end{pmatrix}
= 2 - 1 = 1 \\
M_{44} &= \det \begin{pmatrix}
  1 & 1 & 0 \\
  1 & 2 & 1 \\
  2 & 1 & 3
\end{pmatrix}
= \det \begin{pmatrix}
  2 & 1 \\
  1 & 3
\end{pmatrix}
- \det \begin{pmatrix}
  1 & 1 \\
  2 & 3
\end{pmatrix}
= 6 - 1 - (3 - 2) = 4
\end{align*}

\exercise{2(b)}

\begin{align*}
\det \mat{A} \bigg\rvert_{i=4} &= \sum_{j=1}^{4} (-1)^{4 + j} A_{4j} M_{4j} \\
&= -A_{41} M_{41} + A_{42} M_{42} - A_{43} M_{43} + A_{44} M_{44} \\
&= 4M_{44} - M_{43} = 4(4) - 1 = 15 
\end{align*}

\exercise{2(c)}

\begin{align*}
\det \mat{A} \bigg\rvert_{i=1} &= \sum^{4}_{j=1} (-1)^{1+j} A_{1j} M_{1j} \\
&= M_{11} - M_{12} 
= \det \begin{pmatrix} 2 & 1 & 0 \\ 1 & 3 & 1 \\ 0 & 1 & 4  \end{pmatrix}
- \det \begin{pmatrix} 1 & 1 & 0 \\ 2 & 3 & 1 \\ 0 & 1 & 4  \end{pmatrix} \\
&= 2 \det \begin{pmatrix} 3 & 1 \\ 1 & 4 \end{pmatrix} 
- \det \begin{pmatrix} 1 & 1 \\ 0 & 4 \end{pmatrix} 
- \det \begin{pmatrix} 3 & 1 \\ 1 & 4 \end{pmatrix}
+ \det \begin{pmatrix} 2 & 1 \\ 0 & 4  \end{pmatrix} \\
&= 2 (11) - 4 - (11) + 8 = 15.
\end{align*}

\exercise{2(d)}

\begin{align*}
\det \mat{B} &= \det \begin{pmatrix} 1 & 1 & 0 \\ -1 & 3 & 1 \\ 0 & 1 & 4 \end{pmatrix} 
= \det \begin{pmatrix} 3 & 1 \\ 1 & 4 \end{pmatrix}
- \det \begin{pmatrix} -1 & 1 \\ 0 & 4 \end{pmatrix} \\
&= 11 - (-4) = 15
\end{align*}

Because $\mat{P}$ is triangular,
\begin{align*}
\det \mat{P} = \prod_{i=1}^{n} P_{ii} = -15
\end{align*}

\exercise{2(e)}

\begin{equation*}
\mat{E}_{1} = \begin{pmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & -4 \\
  0 & 0 & 0 & 1
\end{pmatrix} 
\hspace{3em}
\mat{E}_{2} = \begin{pmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 1 & 0 \\
  0 & 0 & 0 & 1
\end{pmatrix} 
\hspace{3em}
\mat{E}_{3} = \begin{pmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 1 & 0
\end{pmatrix}
\end{equation*}

\exercise{2(f)}

As $\det \mat{AB} = \det \mat{A} \det \mat{B}$, then
\begin{equation*}
\det \mat{B} = \frac{ \det \mat{P} }{ \det \mat{E}_{1} \det \mat{E}_{2} \det \mat{E}_{3} } = \frac{15}{ (1)(1)(-1) } = -15
\end{equation*}

\section{Chapter 2: Invertibility}

\subsection{Lab 7: Singularity}

\notes

\begin{enumerate}
\item Given a linear system $\mat{A}x = \mat{B}$, if $\mat{A}$ is invertible, then the system has exactly one solution: $x = \mat{A}^{-1} \mat{B}$.
\item If the constants in a linear system ($\mat{Ax = B}$) are all zero (i.e., $\mat{B = 0}$), we call the linear system a homogenous linear system, and these systems always have at least one solution: $\mat{x = 0}$.
\end{enumerate}

\exercise{2(a)}

$\mat{A}$ is invertible because $\det \mat{A} \neq 0$. As
$$\mat{A}^{-1} = 
\begin{pmatrix} 
  0 & -2 & 1 \\
  \frac{1}{2} & 1 & -\frac{1}{2} \\
  -\frac{1}{6} & 0 & \frac{1}{6}
\end{pmatrix},$$
then
$$\mat{x} = \mat{A}^{-1}\mat{B} =
\begin{pmatrix}
-20 \\ 27/2 \\ -7/6
\end{pmatrix}.$$

\exercise{2(b)}

Likewise, for $\mat{Ax} = \mat{0}, \mat{x} = \mat{0}$, the trivial solution.

\theorem{17}{The inverse of a nonsingular upper triangular matrix is upper triangular.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be a nonsingular upper triangular matrix. Since $\mat{A}$ is invertible, there exists a matrix $\mat{A}^{-1}$ such that $\mat{A} \mat{A}^{-1} = \mat{I}$.

Recall that the product of two upper triangular matrices is upper triangular. Moreover, the inverse of a nonsingular matrix is a product of elementary matrices. Among the elementary matrices, only those corresponding to row operations that preserve upper triangular form (scaling a row, adding a multiple of a row to another row above it, or swapping rows above the diagonal, which are disallowed in upper triangular matrices anyway) are involved in forming an upper triangular matrix.

Since $\mat{A}$ is upper triangular, its Gaussian elimination to the identity matrix involves only upper triangular-preserving row operations. Therefore, the inverse $\mat{A}^{-1}$, being the product of the inverses of those elementary matrices, also consists of upper triangular matrices.

Since a product of upper triangular matrices is upper triangular, it follows that $\mat{A}^{-1}$ is upper triangular.
\end{proof}

\theorem{18}{The inverse of a nonsingular diagonal matrix is diagonal.}

\begin{proof}
Let $\mat{A}$ be an invertible/nonsingular diagonal matrix. Therefore, $\mat{A} \in \mathbb{R}^{n \times n}$ and $\mat{A}$ can be expressed as the product of elementary matrices, $\mat{E}$. As $\mat{I}$ is diagonal, the only elementary operations that can occur within $\mat{E}$ must maintain the diagonal property (with any operations that violate this property at any point being later undone). Similarly, $\mat{A}^{-1} = \mat{E}^{-1}$, and as the inverse of an elementary product is an elementary product of the inverse operations, the diagonal property must be maintained through $\mat{E}^{-1}$ as well.   
\end{proof}

\theorem{19}{The determinant of the inverse is equal to the multiplicative inverse of the determinant, or $\det \mat{A}^{-1} = (\det \mat{A})^{-1}$.}

\begin{proof}
We use the multiplicative property of the determinant: for any \( n \times n \) matrices \( \mat{A} \) and \( \mat{B} \),
\[
\det(\mat{A} \mat{B}) = \det(\mat{A}) \cdot \det(\mat{B}).
\]

Suppose \( \mat{A} \) is an invertible \( n \times n \) matrix. Then \( \mat{A}^{-1} \mat{A} = \mat{I} \), where \( \mat{I} \) is the identity matrix. Taking the determinant of both sides, we get:
\[
\det(\mat{A}^{-1} \mat{A}) = \det(\mat{I}) = 1.
\]

Using the multiplicative property:
\[
\det(\mat{A}^{-1}) \cdot \det(\mat{A}) = 1.
\]

Solving for \( \det(\mat{A}^{-1}) \), we obtain:
\[
\det(\mat{A}^{-1}) = \frac{1}{\det(\mat{A})}.
\]

\end{proof}

\theorem{20}{$\mat{A}$ is invertible if and only if $\mat{A}$ can be written as a product of elementary matrices.}

\begin{proof}
For $\mat{A}$ to be invertible, there must exist some matrix $\mat{E}$ as the product of elementary matrices such that $\mat{EA = I}$, with $\mat{E} = \mat{A}^{-1}$. Therefore, $\mat{A}^{-1}$ is a product of elementary matrices, and as the inverse of an elementary matrix is elementary, the inverse of the product of elementary matrices is also the product of elementary matrices. Therefore, as $\mat{A} = \mat{E}^{-1}$, $\mat{A}$ can be expressed as the prodct of elementary matrices.
\end{proof}

\theorem{21}{If $\mat{A}$ is an $n \times n$ invertible matrix, then the system $\mat{Ax} = \mat{B}$ has exactly one solution for all $n \times 1$ vectors $\mat{B}$.}

\begin{proof}
By definition, for $\mat{Ax=B}$ to have $\ge 1$ solution, $\mat{A}$ must be either underrepresented ($n \times m$) or contradicting ($\det \mat{A} = 0$). Let $\mat{A} \in \mathbb{R}^{n \times n}$ be invertible. Therefore, $\mat{A}$ cannot contradict (meaning that, when represented as linear equations, the equations either state the same information or directly state conflicting information) as $\det \mat{A} \ne 0$. Additionally, as $\mat{A}$ is $n \times n$, $\mat{A}$ cannot be underrepresented: all values can be calculated. Therefore, $\mat{x}$ must have exactly one solution.
\end{proof}

\theorem{22}{If $\mat{A}$ is an $n \times n$ matrix and the system $\mat{Ax} = \mat{B}$ is consistent (has at least one solution) for all $n \times 1$ vectors $\mat{B}$, then $\mat{A}$ is invertible.}

\begin{proof}
As $\mat{Ax=B}$ is consistent, it cannot contain a contradiction (two of the internal linear equations stating conflicting information), because if there was a contradiction, $\mat{Ax=B}$ would yield no solutions and $\det \mat{A} = 0$, making $\mat{A}$ singular. Therefore, $\mat{A}$ must be invertible to have $\ge 1$ solution to $\mat{Ax = B}$.  
\end{proof}

\theorem{23}{If $ad - bc \neq 0$ then $\begin{pmatrix} a & b \\ c & d \end{pmatrix}^{-1} = \frac{1}{(ad - bc)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$.}

\begin{proof}
    This can be verified by direct calculation. Let $\mat{A} = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$. By gaussian elimination:
\begin{align*}
  (\mat{A|I})
= \begin{pmatrix} a & b & 1 & 0 \\ c & d & 0 & 1 \end{pmatrix}
%
&\xrightarrow{R_2 \leftarrow R_2 - \frac{c}{a} R_1}
\begin{pmatrix}
a & b & 1 & 0 \\
0 & d - \frac{cb}{a} & -\frac{c}{a} & 1
\end{pmatrix} \\
%
&\xrightarrow{\substack{R_1 \leftarrow \frac{1}{a} R_1 \\ R_2 \leftarrow \frac{a}{ad - bc} R_2}}
\begin{pmatrix}
1 & \frac{b}{a} & \frac{1}{a} & 0 \\
0 & 1 & \frac{-c}{ad - bc} & \frac{a}{ad - bc}
\end{pmatrix} \\
%
&\xrightarrow{R_1 \leftarrow R_1 - \frac{b}{a} R_2}
\begin{pmatrix}
1 & 0 & \frac{d}{ad - bc} & \frac{-b}{ad - bc} \\
0 & 1 & \frac{-c}{ad - bc} & \frac{a}{ad - bc}
\end{pmatrix}.
\end{align*}
Therefore,
\begin{equation*}
\mat{A}^{-1} =
\begin{pmatrix}
\frac{d}{ad - bc} & \frac{-b}{ad - bc} \\
\frac{-c}{ad - bc} & \frac{a}{ad - bc}
\end{pmatrix}
= \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\end{equation*}
\end{proof}

\theorem{24}{$\mat{A}$ is an $n \times n$ invertible matrix if and only if the system $\mat{Ax} = \mat{0}$ has only trivial solution.}

\begin{proof}
Suppose \( \mat{A} \in \mathbb{R}^{n \times n} \) is invertible. Then, by Theorem 21, for any \( \vec{b} \in \mathbb{R}^n \), the equation \( \mat{Ax} = \vec{b} \) has a unique solution. In particular, taking \( \vec{b} = \vec{0} \), the homogeneous equation \( \mat{Ax} = \vec{0} \) must also have a unique solution. Since \( \vec{x} = \vec{0} \) is always a solution to \( \mat{Ax} = \vec{0} \), it must be the only solution — the solution is trivial.

Conversely, suppose the equation \( \mat{Ax} = \vec{0} \) has only the trivial solution \( \vec{x} = \vec{0} \). Then the null space of \( \mat{A} \) is trivial, so the columns of \( \mat{A} \) are linearly independent. For an \( n \times n \) matrix, linear independence of columns implies that \( \mat{A} \) has full rank \( n \), and hence is invertible.

Therefore, \( \mat{A} \) is invertible if and only if the homogeneous equation \( \mat{Ax} = \vec{0} \) has only the trivial solution.
\end{proof}

\subsection{Lab 8: Modularity and $Z_{p}$}

\notes

\begin{enumerate}
  \item If $x$ and $y$ are integers, they are said to be congruent modulo $p$ (written $x \equiv y \pmod{p}$), meaning that $\frac{x - y}{p}$ is an integer.
\item $r = n \pmod{d}$ can be interpreted as the integer remainder of $n/d$ where $n, d$ and $r$ are all integers.
\item $Z_{p}$ is the set of all integers that can result from $\pmod{p}$ (not considering congruency), and for integer value $p$, this set is all integers $Z_{p} = \{ 0, 1, \dots, p-2, p-1 \}$. 
\end{enumerate}

\exercise{1(a)}

Given $Z_{5} = \{0, 1, 2, 3, 4 \}$, the corresponding additive inverses would be $(0, 4, 3, 2, 1)$ (as each indexwise matchup would thereby result in a modular congruency with 0, such as $1 + 4 \equiv 0 \pmod{5}$).

\exercise{1(b)}

The corresponding multiplicative inverses for $x \ge 0 \in Z_{5}$ would be $(\textit{undefined}, 1, 3, 2, 4)$, as, for example, the integer remainder of $\frac{4 \times 4}{5}$ is 1. 

\exercise{2(a)}

Let $\mat{A} = \begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix}. \mat{A} \equiv \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \pmod{2},$ thus $\det \mat{A} \pmod{2} \neq 0$, therefore $\mat{A}$ is invertible modulo 2. Similarly, $\mat{A} \pmod{5} = \mat{A}$ as all entries of $\mat{A} < 5$. Therefore, as $\det \mat{A} = \det \mat{A} \pmod{5} \ne 0, \mat{A}$ is invertible modulo 5.

\exercise{2(b)}

Let $\mat{A} = \begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix}$ and $\mat{B} = \begin{pmatrix} 4 \\ 1 \end{pmatrix} \pmod{2}.$ $\mat{A} \equiv \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \pmod{2},$ so $\mat{A}^{-1} = \begin{pmatrix} 1 & 0 \\ -1 & 1 \end{pmatrix} \pmod{2}$, therefore
\begin{equation*}
\mat{x}\pmod{2} = \mat{A}^{-1} \mat{B}\pmod{2} = \begin{pmatrix} 4 \\ -3 \end{pmatrix} \pmod{2}.
\end{equation*}
Similarly, the calculation modulo 5 is just the standard calculation (as all values are $< 5$), so
\begin{equation*}
\mat{x}\pmod{5} = \mat{A}^{-1} \mat{B} = \begin{pmatrix} -10 \\ 17 \end{pmatrix} \equiv \begin{pmatrix} 0 \\ 2 \end{pmatrix} \pmod{5}.
\end{equation*}

\subsection{Lab 9: Complex Entries and Eigenvalues}

\notes

\begin{enumerate}
\item Where a complex number is defined with a real and imaginary part in form $a + bi$, its complex conjugate will have the imaginary coefficient as the multiplicative inverse, $a - bi$.
\item The complex conjugate of a matrix with complex entries has all of the entries of the matrix as the complex conjugates of the original matrix's entires.
\item For an $n \times n$ matrix $\mat{A}$, the eigenvalues $\lambda$ of $\mat{A}$ are values of $\lambda$ which satisfy 
\begin{equation}
\mat{Ax} = \lambda \mat{x}
\end{equation}
for each eigenvalue's corresponding eigenvector $\mat{x}$, and the characteristic equation,
\begin{equation}
\det ( \mat{A} - \lambda \mat{I} ) = 0.
\end{equation}
\begin{enumerate}
\item The eigenvalues of a diagonal matrix are just the entries along the diagonal.
\item Singular matrices always have 0 as an eigenvalue, at least (they may have more).
\item The eigenvalues of a matrix $\mat{M}$ are the same as the eigenvalues of its transposition $\mat{M}^{T}$:
\begin{equation}
\eig \mat{M} = \eig \mat{M}^{T}
\end{equation}
\item The eigenvalues of a complex-valued matrix $\mat{M}$ are the same as the eigenvalues of its complex transposition $\overline{\mat{M}}^{T}$:
\begin{equation}
\eig \mat{M} = \eig \mat{\overline{M}}^{T}
\end{equation}
\end{enumerate}
\item A matrix $\mat{M}$ is considered ``Hermetian'' if it is equal to its complex transpose: $\mat{M} = \mat{\overline{M}}^{T}$.
\item A matrix is considered ``Unitary'' if its complex transpose is its inverse: $\mat{M}^{-1} = \mat{\overline{M}}^{T}$.
\end{enumerate}

\exercise{2(a)}

Let $\mat{A} = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}$. $\eig \mat{A} = \{ \lambda \}$ where
\begin{align*}
\det(\mat{A} - \lambda \mat{I}) 
&= \det \begin{pmatrix} 2 - \lambda & 0 \\ 0 & 3 - \lambda \end{pmatrix} \\
&= (2-\lambda)(3-\lambda) = 0 \\
\end{align*}
$\therefore \eig \mat{A} = \{ 2,3 \}$, which is the values on the diagonal.

\exercise{2(b)}

\conjecture{The eigenvalues of any diagonal matrix are the values along the main diagonal.}

\begin{proof}
The eigenvalues must be the values along the main diagonal, as if any of these values were to become 0, the matrix would become singular. This is because of the telescoping property of the determinant, in that for any diagonal matrix $\mat{M} \in \mathbb{R}^{n \times n}$,
\begin{equation*}
\det \mat{M} = \prod_{i = 0}^{n} \mat{M}_{ii},
\end{equation*}
thus if any of these values is 0, $\det \mat{M} = 0$, breaking invertibility.
\end{proof} 

\exercise{2(c)}

Let $\mat{A} = \begin{pmatrix} 1 & 4 \\ 2 & 3 \end{pmatrix}$.
\begin{align*}
\det (\mat{A} - \lambda \mat{I})
&= \det \begin{pmatrix} 1 - \lambda & 4 \\ 2 & 3 - \lambda \end{pmatrix} \\
&= (1 - \lambda)(3 - \lambda) - 8 \\
&= \dots \\
&= (\lambda - 5)(\lambda + 1) = 0 \\
\det (\mat{A}^{T} - \lambda \mat{I})
&= \det \begin{pmatrix} 1 - \lambda & 2 \\ 4 & 3 - \lambda \end{pmatrix} \\
&= \dots \\
&= (\lambda - 5)(\lambda + 1) = 0
\end{align*}
$\therefore \eig \mat{A} = \eig \mat{A}^{T} = \{ -1, 5 \}$.

\exercise{2(d)}

As
\begin{equation*}
\mat{A} = \begin{pmatrix} 1 & -i \\ 2i & i \end{pmatrix}, \hspace{2em} \text{ and } \hspace{2em} \ct{A} = \begin{pmatrix} 1 & -2i \\ i & -i \end{pmatrix},
\end{equation*}
As $\eig \mat{A} = \{\lambda: \det(\mat{A} - \lambda \mat{I}) = 0 \}$, $\eig \mat{A} = \eig \ct{A} = \{ \frac{1 - i \pm \sqrt{8 - 6i} }{2}  \}$. 

This is similar to the properties seen in example 1(c), as the transposition alters the order of elements in such a way that for real numbers, this makes no change to the determinant, but for complex values, the sign will be altered ($\det \mat{A} = \det \mat{A}^{T}$ holds true for the matrix of real values $\mat{A}$ but not for a complex-valued matrix). Therefore, the complex conjugate needs to be taken to fix alter the signs such that it's once again equivalent.

\exercise{2(e)}

An example of a hermitian matrix with complex entries would be
\begin{equation*}
\mat{H} = \begin{pmatrix} 1 & 2-i \\ 2+i & 1 \end{pmatrix}.
\end{equation*}

\exercise{2(f)}

\conjecture{Where $\mat{A}$ is Hermitian, $\eig \mat{A} = \eig \ct{A}$.}

\begin{proof}
$\mat{A}$ is Hermitian, so $\mat{A} = \ct{A}$.
\end{proof}

\exercise{2(g)}

An example of a unitary matrix is
\begin{equation*}
\mat{U} = \frac{1}{\sqrt{2}}  \begin{pmatrix} i & -1 \\ 1 & -i \end{pmatrix}.
\end{equation*}

\exercise{2(h)}

\conjecture{The eigenvalues of the complex transpose of a unitary matrix are the inverses of the eigenvalues of the matrix ($\eig \mat{U} = (\eig \mat{U}^{-1})^{-1}$).}

\begin{proof}
As $\det \mat{A}^{-1} = (\det \mat{A})^{-1}$ for any $\mat{A} \in \mathbb{R}^{n \times n}$, $\eig \mat{A}^{-1} = (\eig \mat{A})^{-1}$. Therefore, for any unitary matrix $\mat{U} \in \mathbb{R}^{n \times n}$, as $\mat{U}^{-1} = \ct{U}$, $\eig \mat{U} = (\eig \ct{U})^{-1}$.
\end{proof}

\theorem{25}{Determinants and complex conjugates are commutative, or the determinant of the complex conjugate is equal to the complex conjugate of the determinant. I.E.,
\begin{equation*}
\det \overline{\mat{A}} = \overline{\det \mat{A}}
\end{equation*}.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}.$ $\det \mat{A}$ is expressed as a combination of the values of $\mat{A}$ via multiplication and division in form
\begin{equation*}
\det \mat{A} = a_{11} ( a_{22} \det (\dots) - a_{23} \det (\dots) + \dots ) - a_{12} ( a_{21} \det(\dots) - \dots ) + \dots,
\end{equation*}
and as taking the complex conjugate is commutative with addition and multiplication (and thus subtraction and division)
\begin{equation*}
\overline{ab} = \overline{a} \times \overline{b} \hspace{2em} \overline{a+b} = \overline{a} + \overline{b},
\end{equation*}
taking the complex conjugate of the determinant is equivalent to calculating the determinant with the complex conjugate of each of the internal values: 
\begin{equation*}
\overline{\det \mat{A}} = \overline{a_{11}} ( \overline{a_{22}} \det (\dots) - \overline{a_{23}} \det (\dots) + \dots ) - \overline{a_{12}} ( \overline{a_{21}} \det(\dots) - \dots ) + \dots,
\end{equation*}
which would be the same as taking the determinant of the complex conjugate of $\mat{A}$, or
\begin{equation*}
\overline{\det \mat{A}} = \det \overline{\mat{A}}.
\end{equation*}
\end{proof}

\theorem{26}{If $\mat{A}$ is invertible, then inversion is commutative with the complex conjugate. I.E., 
\begin{equation*}
\left(\overline{\mat{A}}\right)^{-1} = \overline{\mat{A}^{-1}}
\end{equation*}}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be an invertible matrix, implying that $\mat{A}$ can be expressed as the product of elementary matrices $\mat{E}$. Similarly, $\mat{A}^{-1}$ is expressed as the $\mat{E}^{-1}$, the the product of the inverse elementary operations comprising $\mat{A}$, and because the values that comprise $\mat{A}^{-1}$ can be expressed as the inverses of the elementary operations comprising $\mat{A}$, the values of $\mat{A}^{-1}$ can be expressed as combinations of the elements of $\mat{A}$ using basic operations: addition, subtraction, multiplication, and division. Because taking the complex conjugate applied before or after any basic operation yields the same result, taking the complex conjugate before or after taking the inverse will yield the same value. 
\end{proof}

\theorem{27}{If $c$ is a complex number, $\overline{c\mat{A}} = c\overline{\mat{A}}$.}

\begin{proof}
This is untrue as for any complex scalar $c$ and matrix $\mat{A} \in \mathbb{R}^{m \times n}$, because as scalar multiplication is distributed across all elements, and as for any two complex-valued numbers $a$ and $b$, $\overline{ab} \ne a \overline{b}$ Therefore, $\overline{c} \times \overline{\mat{M}}$
\end{proof}

\theorem{28}{The eigenvalues of a diagonal matrix are the entries on the diagonal.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$ be a diagonal matrix.
\begin{equation*}
\det \mat{A} = \prod_{i=1}^{n} A_{ii}.
\end{equation*}
Thus, if any value in $\{A_{11}, \dots, A_{nn}\}$ is 0, $\det \mat{A} = 0$, breaking invertbility. Therefore, all values of the main diagonal are eigenvalues, as all values satisfy
\begin{equation*}
\det( \mat{A} - \lambda \mat{I} ) = 0
\end{equation*} 
\end{proof}

\theorem{29}{All eigenvalues of Hermitian matrices are real numbers.}

% SHAMELESSLY COPIED FROM CHATGPT!
%
% I genuinely did not know how to prove this.

\begin{proof}
Let \( \mat{H} \in \mathbb{C}^{n \times n} \) be a Hermitian matrix, so \( \mat{H} = \mat{H}^\dagger \), meaning \( \mat{H} \) equals its conjugate transpose.

Suppose \( \lambda \in \mathbb{C} \) is an eigenvalue of \( \mat{H} \) with corresponding (nonzero) eigenvector \( \va{x} \in \mathbb{C}^n \), so:
\[
\mat{H} \va{x} = \lambda \va{x}.
\]

Take the Hermitian inner product of both sides with \( \vec{x} \):
\[
\ct{x} \mat{H} \va{x} = \ct{x} (\lambda \va{x}) = \lambda \ct{x} \va{x}.
\]

Now, note two facts:
\begin{enumerate}
    \item \( \ct{x} \mat{H} \va{x} \) is a \textbf{real number}, because \( \mat{H} \) is Hermitian.
    \item \( \ct{x} \va{x} \) is real and positive, since \( \va{x} \ne 0 \).
\end{enumerate}

Therefore, \( \lambda \ct{x} \va{x} \) is real, which implies that \( \lambda \) must be real.
\end{proof}

\theorem{30}{The complex conjugate of a Hermitian matrix is a Hermitian matrix.}

\begin{proof}
Let $\mat{H} \in \mathbb{R}^{n \times n}$ be a hermitian matrix. Therefore, $\mat{H} = \ct{H}$.
\end{proof}

\theorem{31}{$\mat{A}$ is Unitary if and only if $\mat{A}^{-1} = \ct{A}$}

\begin{proof}
This is true by the definition of a unitary matrix, as to be unitary, $\mat{M} \ct{M} = \ct{M} \mat{M} = \mat{I}$, and similarly, $\mat{M} \mat{M}^{-1} = \mat{M}^{-1} \mat{M} = \mat{I}$, therefore, $\mat{M}^{-1} = \ct{M}$ by the associative property.
\end{proof}

\theorem{32}{$\lambda$ is an eigenvalue of matrix $\mat{A}$ if and only if it is an eigenvalue of $\ct{A}$.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$. As $\det \mat{A} = \det \ct{A}$, then therefore, $\det(\mat{A} - \lambda \mat{I}) = \det(\ct{A} - \lambda \mat{I})$ as $\ct{\mat{A} - \lambda \mat{I}} = \ct{A} - \overline{\lambda} \mat{I}$. Therefore, as $\eig \mat{A} = \{ \lambda | \det(\mat{A} - \lambda \mat{I}) = 0 \}, \eig \mat{A} = \eig \ct{A}$ and vice versa.
\end{proof}

\subsection{Lab 10: Linear Combinations and Dependency}

\notes

\begin{enumerate}
\item If $S = \{ \mat{v}_{1}, \mat{v}_{2}, \dots, \mat{v}_{n} \}$ is a set of vectors (or perhaps, equivalently, a vector of vectors, making $\mat{S}$ a matrix) and there exists scalars within some vector $\mat{k} = (k_{1}, k_{2}, \dots, k_{n})$ such that
\begin{equation}
\mat{w} = \sum_{i=1}^{n} k_{i} \mat{v}_{i} = \mat{Sk},
\end{equation}
then we say that $\mat{w}$ is a linear combination of all $\mat{v} \in \mat{S}$.
\item For set $\mat{S} = \{ \mat{v}_{1}, \dots, \mat{v}_{n} \}$, $\mat{S}$ is linearly independent if
\begin{equation}
\mat{0} = \sum_{i=1}^{n} k_{i} \mat{v}_{i} = \mat{Sk}
\end{equation}
has only the trivial solution, $\mat{k = 0}$. Otherwise, it's linearly dependent, meaning that some nonzero value of $\mat{k}$ can result in $\mat{w=0}$.
\item A set of vectors $\mat{S}$ is said to span a vector space $\mat{V}$ if every vector in $\mat{V}$ can be expressed as a linear combination of all of the vectors in $\mat{S}$.
\begin{enumerate}
\item A common example is dimensionality, like $\mathbb{R}^{1}, \mathbb{R}^{2}, \mathbb{R}^{3}, \mathbb{R}^{n}, \mathbb{R}^{n \times n}, \dots$. 
\item The set $\mat{S}_{1} = \{1\}$ spans $\mathbb{R}^{1}$ as all numbers can be written as a product with 1.
\item Similarly, $\mat{S}_{2} = \{(1,0), (0,1)\}$ spans $\mathbb{R}^{2}$ because all vectors within two dimensions can be described as a linear combination of the two.
\end{enumerate}
\end{enumerate}

\theorem{33}{If $\mat{A}$ is invertible, then the rows of $\mat{A}$ are linearly independent.}

\begin{proof}
For $\mat{A}$ to be invertible, it must satisfy $\mat{EA=I}$ for some product of elementary matrices $\mat{E}$ where $\mat{E}=\mat{A}^{-1}$, allowing $\mat{A}$ to reduce down to $\mat{I}$. As the rows of $\mat{I}$ are clearly linearly independent and all valid elementary operations (scalar multiplication, scalar addition, and swapping) can only produce linearly independent matrices, all elementary matrices and subsequent elementary products must be likewise linearly independent, including $\mat{A}$, which must be expressible as a product of elementary operations.    
\end{proof}

\theorem{34}{If $\mat{A}$ is invertible, then the columns of $\mat{A}$ are linearly independent.}

\begin{proof}
Let $\mat{A} \in \mathbb{R}^{n \times n}$. Suppose $\mat{A}$ has linearly dependent columns in form such that for some column vector $\mat{k} \neq \mat{0}$, $\mat{Ak} = \mat{0}$. To be invertible, $\mat{A}$ must satisfy
$$\mat{A}^{-1}\mat{A} = \mat{I},$$
so if we multiply by $\mat{k}$ on both sides, knowing that $\mat{Ik}=\mat{k}$, we obtain
$$\mat{A}^{-1} \mat{0} = \mat{k}$$
or
$$\mat{k} = \mat{0},$$
a contradiction. Therefore, to be invertible, the columns of $\mat{A}$ must be linearly independent.
\end{proof}

\theorem{35}{A set of vectors with only two vectors in it is linearly dependent if one is a scalar multiple of the other.}

\begin{proof}
True by definition. Let $S = \{ \mat{x}, s\mat{x} \}$. For $\mat{k} = ( -s, 1 ), -s\mat{x}+s\mat{x} = \mat{0}$.
\end{proof}

\theorem{36}{A set of vectors is linearly dependent if it contains the zero vector.}

\begin{proof}
This is not true. For example, consider $S=\{ (0,0), (0,1), (1,0) \}$. Despite containing the zero vector $\mat{0}=(0,0)$, there is no $\mat{k}$ such that $\mat{Sk=0}$ where $\mat{k} \ne \mat{0}$.
\end{proof}

\theorem{37 and 38}{If $\mat{A}$ is an $n \times n$ invertible matrix, then the rows and columns of $\mat{A}$ span $\mathbb{R}^{n}$.}

\begin{proof}
As shown, for $\mat{A}$ to be invertible, the set of vectors representing its rows or its columns must be each linearly independent, containing exactly $n$ vectors of size $n$. Therefore, as any set of $n$ linearly independent vectors of size $n$ can span $\mathbb{R}^{n}$, the rows and columns of $\mat{A}$ span $\mathbb{R}^{n}$.    
\end{proof}

\subtheorem{Any set of $n$ linearly independent vectors of size $n$ can span $\mathbb{R}^{n}$.}

\begin{proof}
Let $S = \{ \mat{v}_1, \mat{v}_2, \ldots, \mat{v}_n \} \subset \mathbb{R}^n$ be a set of $n$ linearly independent vectors.

The dimension of $\mathbb{R}^n$ is $n$, meaning any basis of $\mathbb{R}^n$ must consist of exactly $n$ vectors.

Since $S$ has $n$ linearly independent vectors, it satisfies the definition of a basis. Therefore, $S$ spans $\mathbb{R}^n$.
\end{proof}

\section{Chapter 3: Vector Spaces}

\subsection{Lab 11: Vector Spaces and Subspaces}

\notes

\begin{enumerate}
\item A vector space is a non-empty set of vectors on which some form of vector addition and multiplication are defined (essentially, a set of vectors defined with a special set of rules for manipulating them). 
\item Dimensional vector space notation is $\mathbb{R}^{n}$, where $n$ is the length of the enclosed vectors.
\item Vector subspaces are nonempty subsets of vector spaces, essentially special sets of vectors for which the following three categories are met:
\begin{enumerate}
\item It contains the zero vector (the additive identity).
\item It's closed under scalar multiplication, meaning that for any vector $\mat{v} \in W$ in subspace $W \in S$ in space $S$, all scalar multiples of $\mat{v}$ are in $W$: $k\mat{v} \in W$.
\item It's closed under vector addition, again meaning that for any two vectors $\mat{v}$ and $\mat{u}$, $\mat{v} + \mat{u} = \mat{u} + \mat{v} \in W$. 
\end{enumerate}
\end{enumerate}

\exercise{1(a)} $\va{u} + \va{v} = (2,2) \in V.$

\exercise{1(b)} Yes, $\va{0}$.

\exercise{1(c)} No. $\va{u} + \va{v} \ne \va{v} + \va{u}$.

\exercise{1(d)} $ku = \begin{pmatrix} 4 & 0 \\ 0 & 16 \end{pmatrix} \in V$.

\exercise{1(e)} $\va{0}$.

\exercise{1(f)} $1\mat{v} = \begin{pmatrix} v_{1} & 0 \\ 0 & v_{4} \end{pmatrix}$

\exercise{1(g)} No. $1\mat{v} \ne \mat{v}$

\exercise{1(h)} $\mathbb{R}^{n}$. Standard multiplication and addition.

\exercise{1(i)} $V = \mathbb{R}^{n}$. Standard addition. For $\va{u} \in V$ and $k \in \mathbb{R}$, let $k\va{u} = \va{0}$.

\exercise{2(a)} $W \in M_{2 \times 2} = \left\{ \left. \begin{pmatrix} x & 0 \\ 0 & y \end{pmatrix} \right| x,y \in \mathbb{R} \right\}$.

\exercise{2(b)}

The general solution set is $S \in \mathbb{R}^{2} = \left\{ \left. \begin{pmatrix} -2x \\ x \end{pmatrix} \right| x \in \mathbb{R} \right\}$. It's closed under addition and multiplication, because for all $\va{a}, \va{b} \in S,$ $\va{a} + \va{b} \in S$ and for all $k \in \mathbb{R}$, $k\va{a} \in S$, including $\va{0}$. Therefore, $S$ is a subspace of $\mathbb{R}^{2}$.

\exercise{2(c)}

Let $V = \left\{\left. f(x) \right| f(x) \ne \text{undefined}, x \in \mathbb{R} \right\}$ and $f(x), g(x) \in S$ be continuous functions defined over $\mathbb{R}$. As any linear combinations of $f(x)$ and $g(x)$ is still continuous, any linear combinations of the elements in $V$ is still in $V$, including $4+5\cos(x)$. Additionally, let 
$$S \in V = \left\{ \left. c_{1} + c_{2} \cos(x) + c_{3} \sin(x) \right| c_1, c_2, c_3 \in \mathbb{R} \right\}$$
be the set of all linear combinations of $1, \cos(x),$ and $\sin(x)$, for which, each element $f(x) \in S$ can be represented by its associated vector of constants 
$$\va{c}_{f(x)} = (c_1, c_2, c_3),$$
as there is a direct mapping between any constant vector $\va{c} \in \mathbb{R}^{3}$ and its corresponding linear combination function $f(x)$. Therefore, as $\mathbb{R}^{3}$ is closed under addition and scalar multiplication, so is $S$.

\theorem{39}{If $V$ is the set of $2 \times 2$ invertible matrices, then $V$ is a vector space under matrix addition and scalar multiplication of matrices.}

\begin{proof}
Untrue. $V$ is not closed under matrix addition. $\mat{I}, -\mat{I} \in V$ but $\mat{I} + (-\mat{I}) = \mat{0} \not\in V$.
\end{proof}

\theorem{40}{The set of $2 \times 2$ symmetric matrices under matrix addition and scalar multiplication of matrices is a vector space.}

\begin{proof}
Let $V \in M_{2 \times 2}$ be the set of symmetric $2 \times 2$ matrices. Let $\mat{A},\mat{B} \in V$. By rules of symmetry, $\mat{A+B}$ is symmetric, so $\mat{A+B} \in S$. Additionally, by rules of symmetry, for scalar $k \in \mathbb{R}$, $k\mat{A} \in S$. Therefore, $V$ is a vector space. 
\end{proof}

\theorem{41}{If $V$ is a vector space with $\va{u}_1$ and $\va{u}_2$ vectors in $V$ then $a_1 \va{u}_1 + a_2 \va{u}_2 + b_1 \va{u}_1 + b_{2} \va{u}_{2} = (a_1 + b_1) \va{u}_1 + (a_2 + b_2) \va{u}_2$ and for any scalars $a_1, a_2, b_1, b_2$.}

\begin{proof}
Where $V$ is a vector space, $\va{u}_{1} + \va{u}_{2} = \va{u}_{2} + \va{u}_{1}$ and $k\va{u} = (ku_{1}, ku_{2}, \dots, ku_n)$ for any scalar $k$. Therefore, scalar multiplication can be written distributively, $(a+b)\va{u} = a\va{u} + b\va{u}$, and vector addition can be written commutatively. Thus, the assertion is true.
\end{proof}

\theorem{42}{If $\mat{A}$ is an $n \times n$ matrix, then the solution set to $Ax = 0$ is a subspace of $\mathbb{R}^{n}$.}

\begin{proof}
Let $S \in \mathbb{R}^{n} = \left\{\left. \mat{x} \right| \mat{Ax} = 0 \right\}.$ If $\mat{A}$ is invertible, $S = \{\va{0}\}$, which is a valid vector space (the only valid single-item vector space). Where $\mat{A}$ is singular, $S$ will be infinitely large. Let $\va{a}, \va{b} \in S$. $k\va{a} \in S$ be solutions. Because a scalar multiplied anywhere in the chain of matrix multiplication produces the same matrix (see below), $\mat{A}(k\mat{a}) = k(\mat{Aa}) = \mat{0} \in S$. 

Furthermore, all solutions $\mat{x} \in S$ are based on relationships between the values of $\mat{x}$, in such a way that $\mat{x}$ can be described internally as a series of base values,
$$\mat{x} = ( x_1, x_2, \dots, x_n ),$$
and all $\mat{v} \in \mat{S}$ must essentially be a scalar multiple of these base values (or in other words, all vectors in $S$ are scalar multiples of each other). Therefore, for any two vectors $\mat{a}$ and $\mat{b}$ in $S$, $\mat{a+b} \in S$. Therefore, $S$ is a subspace of $\mathbb{R}^{n}$. 
\end{proof}

\subtheorem{$k\mat{A} \mat{B} = \mat{A} k \mat{B}$}

\begin{proof}
Scalar multiplication is element-wise and distributed over the entire matrix, so it can occur before or after matrix multiplication. 
\end{proof}

\theorem{43}{If $\mat{A}$ is an $n \times n$ matrix, then the set of linear combinations of the rows of $\mat{A}$ is a subspace of $\mathbb{R}^{n}$.}

\begin{proof}
Let $\va{v}, \va{u} \in \mat{A}$, $S \in \mathbb{R}^{n}$ be the set of linear combinations of rows in $\mat{A}$, and $k \in \mathbb{R}$ be a scalar. $S$ is closed under addition as $\va{v} + \va{u} = \va{u} + \va{v} \in S$. Additionally, $S$ is closed under scalar multiplication, as all $k\va{u} \in S$. Therefore, $S$ is a subspace of $\mathbb{R}^{n}$.
\end{proof}

\theorem{44}{If $S = \{ \va{v}_1, \va{v}_2, \dots, \va{v}_n \}$ is a set of vectors in vector space $V$, then the set of all linear combinations of vectors in $S$ is a subspace of $V$.}

\begin{proof}
Let $W$ be the set of linear combinations of vectors in $S$, $\va{v}, \va{u} \in S$ and $k \in \mathbb{R}$ be a scalar. As $V$ is a vector space, any $k\mat{v}$ or $\va{v} + \va{u}$ are in $W$, and therefore, in $V$. As $W$ is closed under addition and scalar multiplication, $W$ is a subspace of $V$.
\end{proof}

\subsection{Lab 12: Basis Vectors, Null Space, Rank, Column/Rowspace}

\notes

\begin{enumerate}
\item Where a set of vectors $S$ spans a vector space $V$ if all vectors in $V$ can be written as a linear combination of vectors in $S$, $S$ acts as a basis for $V$ (and the vectors of $S$ are basis vectors of $V$).
\begin{enumerate}
\item An example is the cardinal vectors, $(0,1,0), (1,0,0),$ and $(0,0,1)$, which act as basis vectors for $\mathbb{R}^{3}$.
\end{enumerate}
\item The dimension of a vector space (written $\dim V$) is the number of vectors that form its basis set. If the vector space $V$ consists only of $\mat{0}$, then $\dim V = 0$.
\begin{enumerate}
\item If the number of basis vectors is finite, $V$ is called ``finite dimensional.'' If the number if infinite, $V$ is ``infinite dimensional.''
\end{enumerate}
\item A basis for a vector space is not unique, a vector space can have multiple bases. However, all basis sets for the same vector space must contain the same number of vectors (the dimensionality of the basis).
\item If two vector spaces have the same basis, they are in the same vector space.
\item For a given matrix/vector $\mat{A}$, the solution set $S$ to $\mat{Ax=0}$ is known as the nullspace of $\mat{A}$ and the dimension of this null space $\dim S$ is known as the nullility of $\mat{A}$.
\begin{enumerate}
\item The nullspace is considered empty if the only valid solution is the trivial solution, $\mat{x=0}$.
\item The dimension of the nullspace is considered the nullility of the matrix, $\dim (\text{null } \mat{A}) = \text{nullility } \mat{A}$
\end{enumerate}
\item The rowspace/columnspace of a matrix is the set of all linear combinations from the rows/columns of the matrix:
\begin{gather}
\rowsp \mat{A} = \set{ \sum_{i=1}^{n} k_{i} \va{r}_{i\mat{A}} }{ \va{k} \in \mathbb{R}^{n} }. \\
\colsp \mat{A} = \set{ \sum_{i=1}^{n} k_{i} \va{c}_{i\mat{A}} }{ \va{k} \in \mathbb{R}^{n} }.
\end{gather}
\item The dimensionality of the rowspace or columnspace is called the ``rank'' of the matrix: 
\begin{equation}
\rank \mat{A} = \dim( \rowsp \mat{A} ) = \dim( \colsp \mat{A} )
\end{equation}
and it's defined as the maximum number of linearly independent rows or columns in $\mat{A}$.
\begin{enumerate}
\item The rowspace and columnspace of an $n \times n$ invertible matrix must span $\mathbb{R}^{n}$.
\item Basis vectors of the rowspace of $\mat{A}$ are simply the nonzero row vectors of $\rref( \mat{A} )$, and the basis vectors of the columnspace of $\mat{A}$ are the original columns of $\mat{A}$ corresponding to the columns containing leading ones in $\rref( \mat{A} )$.
\end{enumerate}
\end{enumerate}

\exercise{1(a)} $\{ (0,0,1), (0,1,0), (1,0,0), (1,1,1) \}$

\exercise{1(b)} $\{ (0,0,1), (0,1,0) \}$

\exercise{1(c)} $\{ (0,0,2), (0,2,0), (2,0,0) \}$

\exercise{1(d)} $\dim \mathbb{R}^{n} = n$

\exercise{2(a)} $\basis \null \mat{A} = \left\{ \begin{pmatrix} -2 \\ 1 \end{pmatrix} \right\}$

\exercise{2(b)} 

\end{document}
